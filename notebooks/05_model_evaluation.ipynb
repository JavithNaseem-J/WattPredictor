{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('../')\n",
    "sys.path.append(os.path.join(os.getcwd(), \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from electron.utils.helpers import *\n",
    "from electron.utils.exception import *\n",
    "from electron.constants import *\n",
    "from electron import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelEvaluationConfig:\n",
    "    model_path: Path\n",
    "    x_transform: Path\n",
    "    y_transform: Path\n",
    "    metrics_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath = CONFIG_PATH,\n",
    "                 params_filepath = PARAMS_PATH,\n",
    "                 schema_filepath = SCHEMA_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "\n",
    "        model_evaluation_config =  ModelEvaluationConfig(\n",
    "            model_path=Path(config.model_path),\n",
    "            x_transform=Path(config.x_transform),\n",
    "            y_transform=Path(config.y_transform),\n",
    "            metrics_path=Path(config.metrics_path)\n",
    "        )\n",
    "\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import mlflow\n",
    "import dagshub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "        mlflow.set_experiment(\"Electricity Demand Prediction\")\n",
    "        logger.info(\"MLflow tracking setup complete.\")\n",
    "\n",
    "    def evaluate(self):\n",
    "        try:\n",
    "            # Load model and test data\n",
    "            model = joblib.load(self.config.model_path)\n",
    "            x_test = np.load(self.config.x_transform, allow_pickle=True)\n",
    "            y_test = np.load(self.config.y_transform, allow_pickle=True).squeeze()\n",
    "\n",
    "            # Predict\n",
    "            preds = model.predict(x_test)\n",
    "\n",
    "            # Metrics\n",
    "            metrics = {\n",
    "                \"mse\": mean_squared_error(y_test, preds),\n",
    "                \"mae\": mean_absolute_error(y_test, preds),\n",
    "                \"rmse\": np.sqrt(mean_squared_error(y_test, preds)),\n",
    "                \"mape\": np.mean(np.abs((y_test - preds) / y_test)) * 100 if np.any(y_test != 0) else np.inf,\n",
    "                \"r2_score\": r2_score(y_test, preds),\n",
    "                \"adjusted_r2\": 1 - (1 - r2_score(y_test, preds)) * (len(y_test) - 1) / (len(y_test) - x_test.shape[1] - 1)\n",
    "            }\n",
    "\n",
    "            create_directories([Path(self.config.metrics_path).parent])\n",
    "            save_json(Path(self.config.metrics_path), metrics)\n",
    "            logger.info(f\"Model evaluation metrics: {metrics}\")\n",
    "            # Log to MLflow\n",
    "            with mlflow.start_run(run_name=\"Model Evaluation\"):\n",
    "                mlflow.log_metrics({k: float(v) for k, v in metrics.items()})\n",
    "                mlflow.set_tag(\"stage\", \"evaluation\")\n",
    "                mlflow.log_artifact(self.config.metrics_path)\n",
    "                mlflow.log_artifact(self.config.model_path)\n",
    "\n",
    "            logger.info(\"✅ Model evaluation complete. Metrics logged.\")\n",
    "            return metrics\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 11:15:05,012: INFO: helpers: yaml file: config_file\\config.yaml loaded successfully]\n",
      "[2025-07-06 11:15:05,012: INFO: helpers: yaml file: config_file\\schema.yaml loaded successfully]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 11:15:05,029: INFO: helpers: yaml file: config_file\\params.yaml loaded successfully]\n",
      "[2025-07-06 11:15:05,031: INFO: helpers: created directory at: artifacts]\n",
      "[2025-07-06 11:15:05,060: INFO: 2097186014: MLflow tracking setup complete.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Javith Naseem\\.conda\\envs\\ele\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 11:15:09,555: INFO: helpers: created directory at: artifacts\\model_evaluation]\n",
      "[2025-07-06 11:15:09,555: INFO: helpers: json file saved at: artifacts\\model_evaluation\\metrics.json]\n",
      "[2025-07-06 11:15:09,555: INFO: 2097186014: Model evaluation metrics: {'mse': 581.2932073192122, 'mae': 16.75716600136802, 'rmse': 24.110022963888113, 'mape': 1.4316218193159305, 'r2_score': 0.9997356598514628, 'adjusted_r2': 0.9997331910601854}]\n",
      "[2025-07-06 11:15:09,694: INFO: 2097186014: ✅ Model evaluation complete. Metrics logged.]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluation = ModelEvaluation(config=model_evaluation_config)\n",
    "    model_evaluation.evaluate()\n",
    "except Exception as e:\n",
    "    raise CustomException(str(e), sys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ele",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
