{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0005c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('../')\n",
    "sys.path.append(os.path.join(os.getcwd(), \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6d5c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import optuna\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from WattPredictor.utils.helpers import *\n",
    "from WattPredictor.utils.ts_generator import features_and_target\n",
    "from WattPredictor.config.model_config import ModelConfigurationManager\n",
    "from WattPredictor.utils.feature import feature_store_instance\n",
    "from WattPredictor.entity.config_entity import TrainerConfig\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error,root_mean_squared_error\n",
    "from WattPredictor.utils.helpers import create_directories\n",
    "from WattPredictor.utils.exception import CustomException\n",
    "from WattPredictor.utils.logging import logger\n",
    "\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, config: TrainerConfig):\n",
    "        self.config = config\n",
    "        self.feature_store =feature_store_instance()\n",
    "\n",
    "        self.models = {\n",
    "            \"XGBoost\": {\n",
    "                \"class\": XGBRegressor,\n",
    "                \"search_space\": lambda trial: {\n",
    "                    \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "                    \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                    \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "                }\n",
    "            },\n",
    "            \"LightGBM\": {\n",
    "                \"class\": LGBMRegressor,\n",
    "                \"search_space\": lambda trial: {\n",
    "                    \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 150),\n",
    "                    \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "                    \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def load_training_data(self):\n",
    "        try:\n",
    "            df, _ = self.feature_store.get_training_data(\"elec_wx_features_view\")\n",
    "            df = df[['date', 'demand', 'sub_region_code', 'temperature_2m']]\n",
    "            df.sort_values(\"date\", inplace=True)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def train(self):\n",
    "        try:\n",
    "            df = self.load_training_data()\n",
    "            train_df = df[df['date'] < self.config.cutoff_date]\n",
    "            test_df = df[df['date'] >= self.config.cutoff_date]\n",
    "\n",
    "            train_x, train_y = features_and_target(train_df, self.config.input_seq_len, self.config.step_size)\n",
    "            train_x.drop(columns=[\"date\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "            best_overall = {\"model_name\": None, \"score\": float(\"inf\"), \"params\": None}\n",
    "\n",
    "            for model_name, model_info in self.models.items():\n",
    "                logger.info(f\"Running Optuna HPO for {model_name}\")\n",
    "\n",
    "                def objective(trial):\n",
    "                    params = model_info[\"search_space\"](trial)\n",
    "                    model = model_info[\"class\"](**params)\n",
    "                    x_tr, x_val, y_tr, y_val = train_test_split(train_x, train_y, test_size=0.2, shuffle=False)\n",
    "                    model.fit(x_tr, y_tr)\n",
    "                    preds = model.predict(x_val)\n",
    "                    return mean_squared_error(y_val, preds)\n",
    "\n",
    "                study = optuna.create_study(direction=\"minimize\")\n",
    "                study.optimize(objective, n_trials=self.config.n_trials)\n",
    "\n",
    "                best_params = study.best_params\n",
    "                model = model_info[\"class\"](**best_params)\n",
    "                score = -cross_val_score(model, train_x, train_y, cv=KFold(n_splits=5), scoring=\"neg_root_mean_squared_error\").mean()\n",
    "\n",
    "                if score < best_overall[\"score\"]:\n",
    "                    best_overall.update({\n",
    "                        \"model_name\": model_name,\n",
    "                        \"score\": score,\n",
    "                        \"params\": best_params\n",
    "                    })\n",
    "\n",
    "\n",
    "            final_model_class = self.models[best_overall[\"model_name\"]][\"class\"]\n",
    "            final_model = final_model_class(**best_overall[\"params\"])\n",
    "            final_model.fit(train_x, train_y)\n",
    "\n",
    "            model_path = Path(self.config.root_dir) / self.config.model_name\n",
    "            create_directories([model_path.parent])\n",
    "            save_bin(final_model, model_path)\n",
    "\n",
    "            input_schema = Schema(train_x.head(10))\n",
    "            output_schema = Schema(pd.DataFrame(train_y))\n",
    "            model_schema = ModelSchema(input_schema=input_schema, output_schema=output_schema)\n",
    "\n",
    "\n",
    "            model_registry = self.feature_store.project.get_model_registry()\n",
    "            hops_model = model_registry.python.create_model(\n",
    "                name=\"wattpredictor_\" + best_overall[\"model_name\"].lower(),\n",
    "                metrics = {'rmse':score},\n",
    "                input_example=train_x.head(10),\n",
    "                model_schema=model_schema,\n",
    "                description=\"Best model trained on electricity demand\"\n",
    "            )\n",
    "            hops_model.save(model_path.as_posix())\n",
    "\n",
    "            logger.info(f\"Best model registered: {best_overall}\")\n",
    "            return best_overall\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da36e341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-17 16:56:24,982: INFO: helpers: yaml file: config_file\\config.yaml loaded successfully]\n",
      "[2025-07-17 16:56:24,987: INFO: helpers: yaml file: config_file\\params.yaml loaded successfully]\n",
      "[2025-07-17 16:56:24,991: INFO: helpers: yaml file: config_file\\schema.yaml loaded successfully]\n",
      "[2025-07-17 16:56:24,992: INFO: helpers: created directory at: artifacts]\n",
      "[2025-07-17 16:56:24,995: INFO: helpers: created directory at: artifacts/model_trainer]\n",
      "[2025-07-17 16:56:25,002: INFO: helpers: yaml file: config_file\\config.yaml loaded successfully]\n",
      "[2025-07-17 16:56:25,006: INFO: helpers: yaml file: config_file\\params.yaml loaded successfully]\n",
      "[2025-07-17 16:56:25,009: INFO: helpers: yaml file: config_file\\schema.yaml loaded successfully]\n",
      "[2025-07-17 16:56:25,012: INFO: external: Initializing external client]\n",
      "[2025-07-17 16:56:25,014: INFO: external: Base URL: https://c.app.hopsworks.ai:443]\n",
      "[2025-07-17 16:56:27,770: INFO: python: Python Engine initialized.]\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1240214\n",
      "[2025-07-17 16:56:30,376: INFO: feature_store: Connected to Hopsworks Feature Store: WattPredictor]\n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (10.55s) \n",
      "[2025-07-17 16:56:47,977: WARNING: warnings: VersionWarning: Incremented version to `10`.\n",
      "]\n",
      "[2025-07-17 16:56:47,979: INFO: feature_store: Retrieved training data from Feature View 'elec_wx_features_view'.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating TS features: 100%|██████████| 11/11 [00:00<00:00, 13.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-17 16:56:48,845: INFO: 2787390486: Running Optuna HPO for XGBoost]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2025-07-17 16:56:48,846] A new study created in memory with name: no-name-ced89ad1-056c-48c4-94e1-02a6eb32b150\n",
      "[I 2025-07-17 16:56:55,705] Trial 0 finished with value: 3549718.8096737126 and parameters: {'n_estimators': 231, 'max_depth': 3, 'learning_rate': 0.02016454481201762}. Best is trial 0 with value: 3549718.8096737126.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-17 16:57:23,596: INFO: 2787390486: Running Optuna HPO for LightGBM]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-17 16:57:23,598] A new study created in memory with name: no-name-c0fddb55-da41-4194-8bb8-cb13c2ec47b3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 171553\n",
      "[LightGBM] [Info] Number of data points in the train set: 2138, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 1211.671188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-17 16:57:34,433] Trial 0 finished with value: 3226381.525077507 and parameters: {'num_leaves': 72, 'learning_rate': 0.13759360434821385, 'n_estimators': 77}. Best is trial 0 with value: 3226381.525077507.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013783 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 171553\n",
      "[LightGBM] [Info] Number of data points in the train set: 2138, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 1574.366698\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 171554\n",
      "[LightGBM] [Info] Number of data points in the train set: 2138, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 1742.909261\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 171554\n",
      "[LightGBM] [Info] Number of data points in the train set: 2138, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 1625.504677\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012381 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 171554\n",
      "[LightGBM] [Info] Number of data points in the train set: 2139, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 1703.911173\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 171553\n",
      "[LightGBM] [Info] Number of data points in the train set: 2139, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 1211.360916\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013555 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 171555\n",
      "[LightGBM] [Info] Number of data points in the train set: 2673, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 1571.589226\n",
      "[2025-07-17 16:58:06,796: INFO: helpers: created directory at: artifacts\\model_trainer]\n",
      "[2025-07-17 16:58:06,806: INFO: helpers: binary file saved at: artifacts\\model_trainer\\model.joblib]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1cb008481bd4d0f96e27cd6431a9f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9cf0e69ff384e25a2b79982ed9e0eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading f:\\WattPredictor\\artifacts/model_trainer/model.joblib: 0.000%|          | 0/519677 elapsed<00:00 rem…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "901b296618a7459188442e6def84ce11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading f:\\WattPredictor\\input_example.json: 0.000%|          | 0/5256 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0043e8b087b040bdbf5148200971ceca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading f:\\WattPredictor\\model_schema.json: 0.000%|          | 0/61417 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created, explore it at https://c.app.hopsworks.ai:443/p/1240214/models/wattpredictor_lightgbm/2\n",
      "[2025-07-17 16:58:25,150: INFO: 2787390486: Best model registered: {'model_name': 'LightGBM', 'score': 420.0868412047661, 'params': {'num_leaves': 72, 'learning_rate': 0.13759360434821385, 'n_estimators': 77}}]\n"
     ]
    }
   ],
   "source": [
    "try:    \n",
    "    config = ModelConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer = Trainer(config=model_trainer_config)\n",
    "    model_trainer.train()\n",
    "\n",
    "except Exception as e:\n",
    "    raise CustomException(e, sys) from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af58297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WattPredictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
