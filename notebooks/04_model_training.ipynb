{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0005c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('../')\n",
    "sys.path.append(os.path.join(os.getcwd(), \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdaf3a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from electron.utils.helpers import *\n",
    "from electron.utils.exception import *\n",
    "from electron.constants import *\n",
    "from electron import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bde322be",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    x_transform: Path\n",
    "    y_transform: Path\n",
    "    model_name: str\n",
    "    scoring: str\n",
    "    cv_folds: int\n",
    "    n_jobs: int\n",
    "    n_trials: int\n",
    "    early_stopping_rounds: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c97475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_PATH,\n",
    "                       params_filepath=PARAMS_PATH,\n",
    "                       schema_filepath=SCHEMA_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.model_trainer\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config =  ModelTrainerConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            x_transform=Path(config.x_transform),\n",
    "            y_transform=Path(config.y_transform),\n",
    "            model_name=config.model_name,\n",
    "            scoring=params.scoring,\n",
    "            cv_folds=params.cv_folds,\n",
    "            n_jobs=params.n_jobs,\n",
    "            n_trials=params.n_trials,\n",
    "            early_stopping_rounds=params.early_stopping_rounds,\n",
    "        )\n",
    "\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d6d5c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import mlflow\n",
    "import optuna\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "        mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "        mlflow.set_experiment(\"Electricity Demand Prediction\")\n",
    "        logger.info(\"MLflow tracking setup complete.\")\n",
    "\n",
    "        self.models = {\n",
    "            \"XGBoost\": {\n",
    "                \"class\": XGBRegressor,\n",
    "                \"search_space\": lambda trial: {\n",
    "                    \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "                    \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                    \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "                },\n",
    "                \"mlflow_module\": mlflow.xgboost,\n",
    "            },\n",
    "            \"LightGBM\": {\n",
    "                \"class\": LGBMRegressor,\n",
    "                \"search_space\": lambda trial: {\n",
    "                    \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 150),\n",
    "                    \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "                    \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "                },\n",
    "                \"mlflow_module\": mlflow.lightgbm,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def train(self):\n",
    "        train_x = np.load(self.config.x_transform, allow_pickle=True)\n",
    "        train_y = np.load(self.config.y_transform, allow_pickle=True).squeeze()\n",
    "\n",
    "\n",
    "\n",
    "        best_overall = {\"model_name\": None, \"score\": float(\"inf\"), \"params\": None}\n",
    "\n",
    "        for model_name, model_info in self.models.items():\n",
    "            logger.info(f\"Starting Optuna HPO for {model_name}\")\n",
    "\n",
    "            def objective(trial):\n",
    "                params = model_info[\"search_space\"](trial)\n",
    "                model = model_info[\"class\"](**params)\n",
    "\n",
    "                # Train/val split for early stopping\n",
    "                x_train, x_val, y_train, y_val = train_test_split(\n",
    "                    train_x, train_y, test_size=0.2, shuffle=False\n",
    "                )\n",
    "\n",
    "                if model_name == \"XGBoost\":\n",
    "                    model.set_params(early_stopping_rounds=0, eval_metric=\"rmse\")\n",
    "                    model.fit(x_train, y_train, eval_set=[(x_val, y_val)], verbose=False)\n",
    "                elif model_name == \"LightGBM\":\n",
    "                    model.set_params(early_stopping_rounds=0)\n",
    "                    model.fit(x_train, y_train, eval_set=[(x_val, y_val)], eval_metric=\"rmse\")\n",
    "\n",
    "                preds = model.predict(x_val)\n",
    "                rmse = root_mean_squared_error(y_val, preds)\n",
    "                return rmse\n",
    "\n",
    "            study = optuna.create_study(direction=\"minimize\")\n",
    "            study.optimize(objective, n_trials=self.config.n_trials)\n",
    "\n",
    "            best_params = study.best_params\n",
    "            logger.info(f\"Best params for {model_name}: {best_params}\")\n",
    "\n",
    "            model = model_info[\"class\"](**best_params)\n",
    "            kf = KFold(n_splits=self.config.cv_folds, shuffle=False)\n",
    "            scores = cross_val_score(model, train_x, train_y, cv=kf, scoring=\"neg_root_mean_squared_error\")\n",
    "            mean_score = -scores.mean()\n",
    "\n",
    "            with mlflow.start_run(run_name=f\"{model_name}_best\"):\n",
    "                mlflow.log_params(best_params)\n",
    "                mlflow.log_metric(\"cv_rmse\", mean_score)\n",
    "                mlflow.set_tag(\"model_name\", model_name)\n",
    "\n",
    "            if mean_score < best_overall[\"score\"]:\n",
    "                best_overall.update({\n",
    "                    \"model_name\": model_name,\n",
    "                    \"score\": mean_score,\n",
    "                    \"params\": best_params\n",
    "                })\n",
    "\n",
    "        best_model_class = self.models[best_overall[\"model_name\"]][\"class\"]\n",
    "        final_params = best_overall[\"params\"]\n",
    "        best_model = best_model_class(**final_params)\n",
    "        best_model.fit(train_x, train_y)\n",
    "\n",
    "        model_path = Path(self.config.root_dir) / self.config.model_name\n",
    "        create_directories([model_path.parent])\n",
    "        save_bin(best_model, model_path)\n",
    "\n",
    "        # Log final model\n",
    "        with mlflow.start_run(run_name=f\"{best_overall['model_name']}_final\"):\n",
    "            mlflow.log_params(final_params)\n",
    "            mlflow.log_metric(\"cv_rmse\", best_overall[\"score\"])\n",
    "            mlflow.set_tag(\"stage\", \"final\")\n",
    "\n",
    "        logger.info(f\"Best model: {best_overall}\")\n",
    "        return best_overall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da36e341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 10:05:30,070: INFO: helpers: yaml file: config_file\\config.yaml loaded successfully]\n",
      "[2025-07-06 10:05:30,074: INFO: helpers: yaml file: config_file\\params.yaml loaded successfully]\n",
      "[2025-07-06 10:05:30,078: INFO: helpers: yaml file: config_file\\schema.yaml loaded successfully]\n",
      "[2025-07-06 10:05:30,080: INFO: helpers: created directory at: artifacts]\n",
      "[2025-07-06 10:05:30,080: INFO: helpers: created directory at: artifacts/model_trainer]\n",
      "[2025-07-06 10:05:30,138: INFO: 1860240262: MLflow tracking setup complete.]\n",
      "[2025-07-06 10:05:30,769: INFO: 1860240262: Starting Optuna HPO for XGBoost]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-06 10:05:30,769] A new study created in memory with name: no-name-69b21feb-5316-496a-8b28-4a4531b62c08\n",
      "[I 2025-07-06 10:05:51,188] Trial 0 finished with value: 28.86133972577451 and parameters: {'n_estimators': 74, 'max_depth': 6, 'learning_rate': 0.16310651485547192}. Best is trial 0 with value: 28.86133972577451.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 10:05:51,203: INFO: 1860240262: Best params for XGBoost: {'n_estimators': 74, 'max_depth': 6, 'learning_rate': 0.16310651485547192}]\n",
      "[2025-07-06 10:07:22,992: INFO: 1860240262: Starting Optuna HPO for LightGBM]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-06 10:07:23,025] A new study created in memory with name: no-name-36764848-a3de-44a0-95e1-904b7c0eceba\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] early_stopping_round is set=0, early_stopping_rounds=0 will be ignored. Current value: early_stopping_round=0\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.215136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 171625\n",
      "[LightGBM] [Info] Number of data points in the train set: 58273, number of used features: 674\n",
      "[LightGBM] [Warning] early_stopping_round is set=0, early_stopping_rounds=0 will be ignored. Current value: early_stopping_round=0\n",
      "[LightGBM] [Info] Start training from score 1587.798294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Javith Naseem\\.conda\\envs\\ele\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-07-06 10:07:48,209] Trial 0 finished with value: 26.900460717823087 and parameters: {'num_leaves': 53, 'learning_rate': 0.15758423542934524, 'n_estimators': 132}. Best is trial 0 with value: 26.900460717823087.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 10:07:48,209: INFO: 1860240262: Best params for LightGBM: {'num_leaves': 53, 'learning_rate': 0.15758423542934524, 'n_estimators': 132}]\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.208169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 171624\n",
      "[LightGBM] [Info] Number of data points in the train set: 58273, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 1477.632849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Javith Naseem\\.conda\\envs\\ele\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.191197 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 171625\n",
      "[LightGBM] [Info] Number of data points in the train set: 58273, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 1322.078201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Javith Naseem\\.conda\\envs\\ele\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.207842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 171625\n",
      "[LightGBM] [Info] Number of data points in the train set: 58274, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 1708.010331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Javith Naseem\\.conda\\envs\\ele\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.197559 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 171625\n",
      "[LightGBM] [Info] Number of data points in the train set: 58274, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 1629.085235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Javith Naseem\\.conda\\envs\\ele\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.232115 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 171625\n",
      "[LightGBM] [Info] Number of data points in the train set: 58274, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 1587.802090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Javith Naseem\\.conda\\envs\\ele\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.299074 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 171626\n",
      "[LightGBM] [Info] Number of data points in the train set: 72842, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 1544.922737\n",
      "[2025-07-06 10:09:54,367: INFO: helpers: created directory at: artifacts\\model_trainer]\n",
      "[2025-07-06 10:09:54,386: INFO: helpers: binary file saved at: artifacts\\model_trainer\\model.joblib]\n",
      "[2025-07-06 10:09:54,484: INFO: 1860240262: Best model: {'model_name': 'LightGBM', 'score': 317.30270711594756, 'params': {'num_leaves': 53, 'learning_rate': 0.15758423542934524, 'n_estimators': 132}}]\n"
     ]
    }
   ],
   "source": [
    "try:    \n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer = ModelTrainer(config=model_trainer_config)\n",
    "    model_trainer.train()\n",
    "\n",
    "except Exception as e:\n",
    "    raise CustomException(e, sys) from e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ele",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
