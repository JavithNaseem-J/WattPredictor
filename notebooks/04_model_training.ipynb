{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0005c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('../')\n",
    "sys.path.append(os.path.join(os.getcwd(), \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdaf3a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from WattPredictor.utils.helpers import *\n",
    "from WattPredictor.utils.exception import *\n",
    "from WattPredictor.constants import *\n",
    "from WattPredictor import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bde322be",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    model_name: str\n",
    "    train_features: Path\n",
    "    test_features: Path\n",
    "    x_transform: Path\n",
    "    y_transform: Path\n",
    "    input_seq_len: int\n",
    "    step_size: int\n",
    "    n_trials: int\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FeatureStoreConfig:\n",
    "    hopsworks_project_name: str\n",
    "    hopsworks_api_key: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c97475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_PATH,\n",
    "                       params_filepath=PARAMS_PATH,\n",
    "                       schema_filepath=SCHEMA_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.model_trainer\n",
    "        trans = self.params.transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            train_features= Path(config.train_features),\n",
    "            test_features= Path(config.test_features),\n",
    "            x_transform= Path(config.x_transform),\n",
    "            y_transform= Path(config.y_transform),\n",
    "            model_name=config.model_name,\n",
    "            input_seq_len= trans.input_seq_len,\n",
    "            step_size = trans.step_size,\n",
    "            n_trials=params.n_trials\n",
    "        )\n",
    "\n",
    "        return model_trainer_config\n",
    "    \n",
    "\n",
    "    def get_feature_store_config(self) -> FeatureStoreConfig:\n",
    "\n",
    "        config = self.config.feature_store\n",
    "\n",
    "        feature_store_config = FeatureStoreConfig(\n",
    "                hopsworks_project_name=config.hopsworks_project_name,\n",
    "                hopsworks_api_key=os.environ['hopsworks_api_key'],\n",
    "        )\n",
    "\n",
    "        return feature_store_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "965102a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from WattPredictor.utils.exception import CustomException\n",
    "from WattPredictor import logger\n",
    "\n",
    "class FeatureStore:\n",
    "    def __init__(self, config):\n",
    "        try:\n",
    "            self.config = config\n",
    "            self.connect()\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def connect(self):\n",
    "        try:\n",
    "            self.project = hopsworks.login(\n",
    "                project=self.config.hopsworks_project_name,\n",
    "                api_key_value=self.config.hopsworks_api_key\n",
    "            )\n",
    "            self.feature_store = self.project.get_feature_store()\n",
    "            self.dataset_api = self.project.get_dataset_api()\n",
    "            logger.info(f\"Connected to Hopsworks Feature Store: {self.config.hopsworks_project_name}\")\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def create_feature_group(self, name, df, primary_key, event_time, description):\n",
    "        try:\n",
    "            try:\n",
    "                fg = self.feature_store.get_feature_group(name=name, version=1)\n",
    "                logger.info(f\"Feature Group '{name}' already exists. Inserting data instead.\")\n",
    "                fg.insert(df)\n",
    "            except:\n",
    "                logger.info(f\"Feature Group '{name}' does not exist. Creating new one.\")\n",
    "                fg = self.feature_store.get_or_create_feature_group(\n",
    "                    name=name,\n",
    "                    version=1,\n",
    "                    primary_key=primary_key,\n",
    "                    event_time=event_time,\n",
    "                    description=description,\n",
    "                    online_enabled=False\n",
    "                )\n",
    "                fg.save(df)\n",
    "\n",
    "            logger.info(f\"Feature Group '{name}' created/updated successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def create_feature_view(self, name: str, feature_group_name: str, features: list):\n",
    "        try:\n",
    "            fg = self.feature_store.get_feature_group(name=feature_group_name, version=1)\n",
    "            fv = self.feature_store.get_or_create_feature_view(\n",
    "                name=name,\n",
    "                version=1,\n",
    "                query=fg.select(features),\n",
    "                description=f\"Feature View for {name}\"\n",
    "            )\n",
    "            logger.info(f\"Feature View '{name}' created successfully\")\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def upload_file_safely(self, local_path: str, target_name: str):\n",
    "        \"\"\"\n",
    "        Upload file to Hopsworks dataset storage.\n",
    "        If it already exists, it will be overwritten.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.dataset_api.upload(\n",
    "                local_path,\n",
    "                f\"Resources/wattpredictor_artifacts/{target_name}\",\n",
    "                overwrite=True \n",
    "            )\n",
    "            logger.info(f\"Uploaded file to Feature Store: {target_name}\")\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "    def download_file(self, remote_name: str, local_path: str = None):\n",
    "        \"\"\"\n",
    "        Download a file from Hopsworks dataset storage.\n",
    "\n",
    "        Args:\n",
    "            remote_name: filename in Hopsworks (inside wattpredictor_artifacts)\n",
    "            local_path: optional local path to save the file. If None, saves in current directory.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            target_path = f\"Resources/wattpredictor_artifacts/{remote_name}\"\n",
    "            if local_path is None:\n",
    "                local_path = remote_name\n",
    "\n",
    "            self.dataset_api.download(\n",
    "                target_path,\n",
    "                local_path=local_path,\n",
    "                overwrite=True\n",
    "            )\n",
    "            logger.info(f\"Downloaded file from Feature Store: {remote_name} to {local_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "\n",
    "    def delete_file(self, target_name: str):\n",
    "        \"\"\"\n",
    "        Delete file from Hopsworks dataset storage.\n",
    "        Only use this if you want to clean up files manually.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            full_path = f\"Resources/wattpredictor_artifacts/{target_name}\"\n",
    "            self.dataset_api.delete(full_path)\n",
    "            logger.warning(f\"Deleted file from Feature Store: {target_name}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"File not found or already deleted: {target_name}\")\n",
    "            # Not raising exception here to allow safe cleanup\n",
    "\n",
    "    def get_training_data(self, feature_view_name: str):\n",
    "        try:\n",
    "            fv = self.feature_store.get_feature_view(name=feature_view_name, version=1)\n",
    "            X, y = fv.training_data()\n",
    "            logger.info(f\"Retrieved training data from Feature View '{feature_view_name}'\")\n",
    "            return X, y\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d6d5c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import mlflow\n",
    "import optuna\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error,root_mean_squared_error\n",
    "from WattPredictor.utils.helpers import create_directories\n",
    "from WattPredictor.utils.exception import CustomException\n",
    "from WattPredictor import logger\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost\n",
    "import lightgbm\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig, feature_store_config):\n",
    "        self.config = config\n",
    "        self.feature_store_config = feature_store_config\n",
    "        self.feature_store = FeatureStore(feature_store_config)\n",
    "\n",
    "        mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "        mlflow.set_experiment(\"Electricity Demand Prediction\")\n",
    "\n",
    "        logger.info(\"MLflow tracking setup complete.\")\n",
    "\n",
    "\n",
    "        self.models = {\n",
    "            \"XGBoost\": {\n",
    "                \"class\": XGBRegressor,\n",
    "                \"search_space\": lambda trial: {\n",
    "                    \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "                    \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                    \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "                },\n",
    "                \"mlflow_module\": mlflow.xgboost,\n",
    "            },\n",
    "            \"LightGBM\": {\n",
    "                \"class\": LGBMRegressor,\n",
    "                \"search_space\": lambda trial: {\n",
    "                    \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 150),\n",
    "                    \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "                    \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "                },\n",
    "                \"mlflow_module\": mlflow.lightgbm,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def load_inputs(self):\n",
    "        try:\n",
    "            self.feature_store.dataset_api.download(\"Resources/wattpredictor_artifacts/train_df.csv/train_features.csv\", overwrite=True)\n",
    "            self.feature_store.dataset_api.download(\"Resources/wattpredictor_artifacts/test_df.csv/test_features.csv\", overwrite=True)\n",
    "            train_df = pd.read_csv(self.config.train_features)\n",
    "            test_df = pd.read_csv(self.config.test_features)\n",
    "\n",
    "            return train_df, test_df\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def get_cutoff_indices(self, df: pd.DataFrame, input_seq_len: int, step_size: int):\n",
    "        stop = len(df) - input_seq_len - 1\n",
    "        return [(i, i + input_seq_len, i + input_seq_len + 1) for i in range(0, stop, step_size)]\n",
    "        \n",
    "    def generate_ts_features_and_target(self, ts_data: pd.DataFrame):\n",
    "            \n",
    "            assert set(['date', 'demand', 'sub_region_code', 'temperature_2m']).issubset(ts_data.columns)\n",
    "\n",
    "            region_codes = ts_data['sub_region_code'].unique()\n",
    "            features = pd.DataFrame()\n",
    "            targets = pd.DataFrame()\n",
    "\n",
    "            input_seq_len = self.config.input_seq_len\n",
    "            step_size = self.config.step_size\n",
    "\n",
    "            for code in tqdm(region_codes, desc=\"Transforming TS Data\"):\n",
    "                ts_one = ts_data[ts_data['sub_region_code'] == code].sort_values(by='date')\n",
    "                indices = self.get_cutoff_indices(ts_one, input_seq_len, step_size)\n",
    "\n",
    "                x = np.zeros((len(indices), input_seq_len), dtype=np.float64)\n",
    "                y = np.zeros((len(indices)), dtype=np.float64)\n",
    "                date_hours, temps = [], []\n",
    "\n",
    "                for i, (start, mid, end) in enumerate(indices):\n",
    "                    x[i, :] = ts_one.iloc[start:mid]['demand'].values\n",
    "                    y[i] = ts_one.iloc[mid]['demand']\n",
    "                    date_hours.append(ts_one.iloc[mid]['date'])\n",
    "                    temps.append(ts_one.iloc[mid]['temperature_2m'])\n",
    "\n",
    "                features_one = pd.DataFrame(\n",
    "                    x,\n",
    "                    columns=[f'demand_prev_{i+1}_hr' for i in reversed(range(input_seq_len))]\n",
    "                )\n",
    "                features_one['date'] = date_hours\n",
    "                features_one['sub_region_code'] = code\n",
    "                features_one['temperature_2m'] = temps\n",
    "\n",
    "                targets_one = pd.DataFrame(y, columns=['target_demand_next_hour'])\n",
    "\n",
    "                features = pd.concat([features, features_one], ignore_index=True)\n",
    "                targets = pd.concat([targets, targets_one], ignore_index=True)\n",
    "\n",
    "            return features, targets['target_demand_next_hour']\n",
    "\n",
    "    def train(self):\n",
    "        train_df, test_df = self.load_inputs()\n",
    "        train_x, train_y = self.generate_ts_features_and_target(train_df)\n",
    "        test_x, test_y = self.generate_ts_features_and_target(test_df)\n",
    "        \n",
    "        train_x = train_x.drop(columns=[\"date\"], errors=\"ignore\")\n",
    "        test_x = test_x.drop(columns=[\"date\"], errors=\"ignore\")\n",
    "\n",
    "        logger.info(f'shape of train_x:{train_x.shape}, train_y:{train_y.shape}, test_x:{test_x.shape}, test_y:{test_y.shape}')\n",
    "\n",
    "        test_x.to_parquet(self.config.x_transform)\n",
    "        test_y.to_frame().to_parquet(self.config.y_transform)\n",
    "\n",
    "\n",
    "        self.feature_store.upload_file_safely(self.config.x_transform, os.path.basename(self.config.x_transform))\n",
    "        self.feature_store.upload_file_safely(self.config.y_transform, os.path.basename(self.config.y_transform))\n",
    "\n",
    "        best_overall = {\"model_name\": None, \"score\": float(\"inf\"), \"params\": None}\n",
    "\n",
    "        for model_name, model_info in self.models.items():\n",
    "            logger.info(f\"Starting Optuna HPO for {model_name}\")\n",
    "\n",
    "            def objective(trial):\n",
    "                params = model_info[\"search_space\"](trial)\n",
    "                model = model_info[\"class\"](**params)\n",
    "\n",
    "                x_train, x_val, y_train, y_val = train_test_split(\n",
    "                    train_x, train_y, test_size=0.2, shuffle=False\n",
    "                )\n",
    "\n",
    "                model.fit(x_train, y_train)\n",
    "                preds = model.predict(x_val)\n",
    "                rmse = root_mean_squared_error(y_val, preds)\n",
    "                return rmse\n",
    "\n",
    "            # Create and run the study for this model\n",
    "            study = optuna.create_study(direction=\"minimize\")\n",
    "            study.optimize(objective, n_trials=self.config.n_trials)\n",
    "\n",
    "            best_params = study.best_params\n",
    "            logger.info(f\"Best params for {model_name}: {best_params}\")\n",
    "\n",
    "            model = model_info[\"class\"](**best_params)\n",
    "            kf = KFold(n_splits=5, shuffle=False)\n",
    "            scores = cross_val_score(model, train_x, train_y, cv=kf, scoring=\"neg_root_mean_squared_error\")\n",
    "            mean_score = -scores.mean()\n",
    "\n",
    "            with mlflow.start_run(run_name=f\"{model_name}_best\"):\n",
    "                mlflow.log_params(best_params)\n",
    "                mlflow.log_metric(\"cv_rmse\", mean_score)\n",
    "                mlflow.set_tag(\"model_name\", model_name)\n",
    "\n",
    "            if mean_score < best_overall[\"score\"]:\n",
    "                best_overall.update({\n",
    "                    \"model_name\": model_name,\n",
    "                    \"score\": mean_score,\n",
    "                    \"params\": best_params\n",
    "                })\n",
    "\n",
    "        best_model_class = self.models[best_overall[\"model_name\"]][\"class\"]\n",
    "        final_params = best_overall[\"params\"]\n",
    "        best_model = best_model_class(**final_params)\n",
    "        best_model.fit(train_x, train_y)\n",
    "\n",
    "        model_path = Path(self.config.root_dir) / self.config.model_name\n",
    "        create_directories([model_path.parent])\n",
    "        save_bin(best_model, model_path)\n",
    "\n",
    "        self.feature_store.upload_file_safely(model_path, \"model.joblib\")\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"{best_overall['model_name']}_final\"):\n",
    "            mlflow.log_params(final_params)\n",
    "            mlflow.log_metric(\"cv_rmse\", best_overall[\"score\"])\n",
    "            mlflow.set_tag(\"stage\", \"final\")\n",
    "\n",
    "        logger.info(f\"Best model: {best_overall}\")\n",
    "        return best_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da36e341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-11 14:58:03,533: INFO: helpers: yaml file: config_file\\config.yaml loaded successfully]\n",
      "[2025-07-11 14:58:03,537: INFO: helpers: yaml file: config_file\\params.yaml loaded successfully]\n",
      "[2025-07-11 14:58:03,544: INFO: helpers: yaml file: config_file\\schema.yaml loaded successfully]\n",
      "[2025-07-11 14:58:03,548: INFO: helpers: created directory at: artifacts]\n",
      "[2025-07-11 14:58:03,549: INFO: helpers: created directory at: artifacts/model_trainer]\n",
      "[2025-07-11 14:58:03,553: INFO: external: Initializing external client]\n",
      "[2025-07-11 14:58:03,554: INFO: external: Base URL: https://c.app.hopsworks.ai:443]\n",
      "[2025-07-11 14:58:06,960: INFO: python: Python Engine initialized.]\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1237149\n",
      "[2025-07-11 14:58:09,525: INFO: 468788050: Connected to Hopsworks Feature Store: JavithNaseem]\n",
      "[2025-07-11 14:58:09,550: WARNING: file_store: Malformed experiment '141997247217478997'. Detailed error Yaml file '.\\mlruns\\141997247217478997\\meta.yaml' does not exist.]\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 356, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "  File \"f:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 454, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"f:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1595, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"f:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 1588, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"f:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\mlflow\\utils\\yaml_utils.py\", line 107, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '.\\mlruns\\141997247217478997\\meta.yaml' does not exist.\n",
      "[2025-07-11 14:58:09,563: INFO: 3080091477: MLflow tracking setup complete.]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4588fac5683f4c4fb03163e37bb4a004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: 0.000%|          | 0/999134 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce60f7a52be245c1abf10e5915d79806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: 0.000%|          | 0/1025674 elapsed<00:00 remaining<?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming TS Data: 100%|██████████| 11/11 [00:00<00:00, 28.50it/s]\n",
      "Transforming TS Data: 100%|██████████| 11/11 [00:00<00:00, 27.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-11 14:58:51,848: INFO: 3080091477: shape of train_x:(1969, 674), train_y:(1969,), test_x:(2112, 674), test_y:(2112,)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256a8587aa5d414a8f8bba01d55cbef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading f:\\WattPredictor\\artifacts\\data_transformation\\test_x.parquet: 0.000%|          | 0/5889601 elapsed<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-11 14:58:58,490: INFO: 468788050: Uploaded file to Feature Store: test_x.parquet]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12e68ea5b754390843d554ee5fbc33e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading f:\\WattPredictor\\artifacts\\data_transformation\\test_y.parquet: 0.000%|          | 0/9758 elapsed<00:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-11 14:59:01,231: INFO: 468788050: Uploaded file to Feature Store: test_y.parquet]\n",
      "[2025-07-11 14:59:01,234: INFO: 3080091477: Starting Optuna HPO for XGBoost]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-11 14:59:01,235] A new study created in memory with name: no-name-1475e4e2-3a06-47e7-9bb5-1e9fa4c81d01\n",
      "[I 2025-07-11 14:59:12,740] Trial 0 finished with value: 161.16921158821907 and parameters: {'n_estimators': 210, 'max_depth': 5, 'learning_rate': 0.08986991860815896}. Best is trial 0 with value: 161.16921158821907.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-11 14:59:12,741: INFO: 3080091477: Best params for XGBoost: {'n_estimators': 210, 'max_depth': 5, 'learning_rate': 0.08986991860815896}]\n",
      "[2025-07-11 15:00:07,071: INFO: 3080091477: Starting Optuna HPO for LightGBM]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-11 15:00:07,072] A new study created in memory with name: no-name-e8702ef2-1691-4b53-8861-c73811555e35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014922 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 171515\n",
      "[LightGBM] [Info] Number of data points in the train set: 1575, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 1736.014603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-11 15:00:25,052] Trial 0 finished with value: 113.29673425759209 and parameters: {'num_leaves': 55, 'learning_rate': 0.21092864357878932, 'n_estimators': 240}. Best is trial 0 with value: 113.29673425759209.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-11 15:00:25,053: INFO: 3080091477: Best params for LightGBM: {'num_leaves': 55, 'learning_rate': 0.21092864357878932, 'n_estimators': 240}]\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011986 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 171515\n",
      "[LightGBM] [Info] Number of data points in the train set: 1575, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 1767.797460\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012593 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 171516\n",
      "[LightGBM] [Info] Number of data points in the train set: 1575, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 1812.050794\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008561 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 171516\n",
      "[LightGBM] [Info] Number of data points in the train set: 1575, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 1201.990476\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013116 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 171516\n",
      "[LightGBM] [Info] Number of data points in the train set: 1575, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 1624.984762\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011362 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 171515\n",
      "[LightGBM] [Info] Number of data points in the train set: 1576, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 1736.500635\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013974 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 171517\n",
      "[LightGBM] [Info] Number of data points in the train set: 1969, number of used features: 674\n",
      "[LightGBM] [Info] Start training from score 1628.678517\n",
      "[2025-07-11 15:01:40,843: INFO: helpers: created directory at: artifacts\\model_trainer]\n",
      "[2025-07-11 15:01:40,886: INFO: helpers: binary file saved at: artifacts\\model_trainer\\model.joblib]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621015797fef44cfa333771cc12b696c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading f:\\WattPredictor\\artifacts\\model_trainer\\model.joblib: 0.000%|          | 0/1216576 elapsed<00:00 re…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-11 15:01:46,278: INFO: 468788050: Uploaded file to Feature Store: model.joblib]\n",
      "[2025-07-11 15:01:46,369: INFO: 3080091477: Best model: {'model_name': 'LightGBM', 'score': 426.507156386507, 'params': {'num_leaves': 55, 'learning_rate': 0.21092864357878932, 'n_estimators': 240}}]\n"
     ]
    }
   ],
   "source": [
    "try:    \n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    feature_store_config = config.get_feature_store_config() \n",
    "    model_trainer = ModelTrainer(config=model_trainer_config,feature_store_config=feature_store_config)\n",
    "    model_trainer.train()\n",
    "\n",
    "except Exception as e:\n",
    "    raise CustomException(e, sys) from e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WattPredictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
