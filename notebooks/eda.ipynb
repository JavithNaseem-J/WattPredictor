{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b463292",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e533aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4198b19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from pathlib import Path\\nimport pandas as pd\\n\\n# Start from notebook location, then go one level up\\nBASE_DIR = Path.cwd().parent\\n\\n# Build the path to the CSV file\\ncsv_path = BASE_DIR / \"artifacts\" / \"data_ingestion\" / \"data\" / \"elec_wx_demand.csv\"\\n\\ndf = pd.read_csv(csv_path)\\ndf.dtypes'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Start from notebook location, then go one level up\n",
    "BASE_DIR = Path.cwd().parent\n",
    "\n",
    "# Build the path to the CSV file\n",
    "csv_path = BASE_DIR / \"artifacts\" / \"data_ingestion\" / \"data\" / \"elec_wx_demand.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "df.dtypes'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1935ca93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('../')\n",
    "sys.path.append(os.path.join(os.getcwd(), \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1ae4b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "from WattPredictor.utils.logging import logger\n",
    "from WattPredictor.utils.exception import CustomException\n",
    "from WattPredictor.utils.helpers import create_directories, save_json, load_json\n",
    "from WattPredictor.entity.config_entity import DataIngestionConfig\n",
    "from WattPredictor.utils.feature import feature_store_instance\n",
    "\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=3600)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "load_dotenv()\n",
    "\n",
    "class Ingestion:\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        self.config = config\n",
    "        self.feature_store = feature_store_instance()\n",
    "        self.openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "    def _elec_get_api(self, year, month, day):\n",
    "        return self.config.elec_api, {\n",
    "            \"frequency\": \"hourly\",\n",
    "            \"data[0]\": \"value\",\n",
    "            \"sort[0][column]\": \"period\",\n",
    "            \"sort[0][direction]\": \"desc\",\n",
    "            \"facets[parent][0]\": \"NYIS\",\n",
    "            \"offset\": 0,\n",
    "            \"length\": 5000,\n",
    "            \"start\": f\"{year}-{month:02d}-{day:02d}\",\n",
    "            \"end\": (datetime(year, month, day) + timedelta(days=1)).strftime(\"%Y-%m-%d\"),\n",
    "            \"api_key\": self.config.elec_api_key\n",
    "        }\n",
    "\n",
    "    def _wx_get_api(self, start_date, end_date):\n",
    "        return self.config.wx_api, {\n",
    "            \"latitude\": 40.7128,\n",
    "            \"longitude\": -74.0060,\n",
    "            \"start_date\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"end_date\": end_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"hourly\": [\"temperature_2m\", \"weather_code\", \"relative_humidity_2m\", \"wind_speed_10m\"],\n",
    "            \"timeformat\": \"unixtime\",\n",
    "            \"timezone\": \"America/New_York\"\n",
    "        }\n",
    "\n",
    "    def _fetch_electricity_data(self, year, month, day):\n",
    "        file_path = self.config.elec_raw_data / f\"hourly_demand_{year}-{month:02d}-{day:02d}.json\"\n",
    "        if file_path.exists():\n",
    "            data = load_json(file_path)\n",
    "            if 'response' in data and 'data' in data['response']:\n",
    "                return pd.DataFrame(data['response']['data'])\n",
    "\n",
    "        url, params = self._elec_get_api(year, month, day)\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        create_directories([self.config.elec_raw_data])\n",
    "        save_json(file_path, data)\n",
    "        return pd.DataFrame(data['response']['data']) if 'response' in data and 'data' in data['response'] else pd.DataFrame()\n",
    "\n",
    "    def _fetch_weather_data(self, start_date, end_date):\n",
    "        file_path = self.config.wx_raw_data / f\"weather_data_{start_date.strftime('%Y-%m-%d')}_to_{end_date.strftime('%Y-%m-%d')}.csv\"\n",
    "        if file_path.exists():\n",
    "            return pd.read_csv(file_path)\n",
    "\n",
    "        url, params = self._wx_get_api(start_date, end_date)\n",
    "        responses = self.openmeteo.weather_api(url, params=params)\n",
    "        hourly = responses[0].Hourly()\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"date\": pd.date_range(\n",
    "                start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "                end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "                freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "                inclusive=\"left\"\n",
    "            ),\n",
    "            \"temperature_2m\": hourly.Variables(0).ValuesAsNumpy(),\n",
    "            \"weather_code\": hourly.Variables(1).ValuesAsNumpy(),\n",
    "            \"relative_humidity_2m\": hourly.Variables(2).ValuesAsNumpy(),\n",
    "            \"wind_speed_10m\": hourly.Variables(3).ValuesAsNumpy()\n",
    "        })\n",
    "\n",
    "        create_directories([self.config.wx_raw_data])\n",
    "        df.to_csv(file_path, index=False)\n",
    "        return df\n",
    "\n",
    "    def _prepare_and_merge(self, elec_data_list, weather_df):\n",
    "        elec_df = pd.concat(elec_data_list, ignore_index=True)\n",
    "        elec_df[\"date\"] = pd.to_datetime(elec_df[\"period\"], utc=True)\n",
    "        weather_df[\"date\"] = pd.to_datetime(weather_df[\"date\"], utc=True)\n",
    "\n",
    "        combined_df = pd.merge(elec_df, weather_df, on=\"date\", how=\"inner\")\n",
    "\n",
    "        combined_df.columns = (\n",
    "            combined_df.columns.str.lower()\n",
    "            .str.replace(\"-\", \"_\", regex=False)\n",
    "            .str.replace(\" \", \"_\", regex=False)\n",
    "            .str.strip()\n",
    "        )\n",
    "\n",
    "        combined_df[\"date_str\"] = combined_df[\"date\"].dt.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "        return combined_df\n",
    "\n",
    "    def download(self):\n",
    "        try:\n",
    "            start = pd.to_datetime(self.config.start_date, utc=True)\n",
    "            end = pd.to_datetime(self.config.end_date, utc=True)\n",
    "\n",
    "            current = start\n",
    "            elec_data = []\n",
    "\n",
    "            while current <= end:\n",
    "                year, month, day = current.year, current.month, current.day\n",
    "                df = self._fetch_electricity_data(year, month, day)\n",
    "                if not df.empty:\n",
    "                    elec_data.append(df)\n",
    "                current += timedelta(days=1)\n",
    "\n",
    "            if not elec_data:\n",
    "                logger.warning(\"No electricity data fetched.\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            wx_df = self._fetch_weather_data(start, end)\n",
    "            if wx_df.empty:\n",
    "                logger.warning(\"No weather data fetched.\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            combined_df = self._prepare_and_merge(elec_data, wx_df)\n",
    "\n",
    "            if combined_df.empty:\n",
    "                logger.warning(\"Merged DataFrame is empty after join.\")\n",
    "                return pd.DataFrame()\n",
    "\n",
    "            self.feature_store.create_feature_group(\n",
    "                name=\"elec_wx_demand\",\n",
    "                df=combined_df,\n",
    "                primary_key=[\"date_str\", \"subba\"], \n",
    "                event_time=\"date\",\n",
    "                description=\"Merged electricity demand and weather data for WattPredictor\"\n",
    "            )\n",
    "\n",
    "            # Save locally\n",
    "            create_directories([self.config.data_file.parent])\n",
    "            combined_df.to_csv(self.config.data_file, index=False)\n",
    "            logger.info(f\"Saved combined dataset to {self.config.data_file}\")\n",
    "\n",
    "            return combined_df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in DataIngestion.download: {str(e)}\")\n",
    "            raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddd870c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from WattPredictor.utils.logging import logger\n",
    "from WattPredictor.entity.config_entity import DataValidationConfig\n",
    "from WattPredictor.utils.helpers import create_directories\n",
    "from WattPredictor.utils.exception import CustomException\n",
    "\n",
    "class Validation:\n",
    "    def __init__(self, config: DataValidationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def validate_data_types(self, data: pd.DataFrame, schema: dict):\n",
    "\n",
    "        type_mapping = {\n",
    "            'int': ['int64', 'int32'],\n",
    "            'float': ['float64', 'float32'],\n",
    "            'object': ['object'],\n",
    "            'str': ['object'], \n",
    "        }\n",
    "\n",
    "        for col, expected_type in schema.items():\n",
    "            if col not in data.columns:\n",
    "                continue \n",
    "                \n",
    "            actual_dtype = str(data[col].dtype)\n",
    "            allowed_dtypes = type_mapping.get(expected_type, [expected_type])\n",
    "\n",
    "            if actual_dtype not in allowed_dtypes:\n",
    "                logger.error(f\"Column '{col}': Expected type '{expected_type}', got '{actual_dtype}'\")\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def validate_column_presence(self, data: pd.DataFrame, schema: dict):\n",
    "        all_cols = list(data.columns)\n",
    "        expected_cols = set(schema.keys())\n",
    "        missing_cols = expected_cols - set(all_cols)\n",
    "\n",
    "        if missing_cols:\n",
    "            logger.error(f\"Missing columns: {missing_cols}\")\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def check_missing_values(self, data: pd.DataFrame) -> bool:\n",
    "        missing = data.isnull().sum()\n",
    "        if missing.any():\n",
    "            logger.error(f\"Missing values detected:\\n{missing[missing > 0]}\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "    def validator(self):\n",
    "        data = pd.read_csv(self.config.data_file)\n",
    "        schema = self.config.all_schema\n",
    "\n",
    "        logger.info(f\"Starting validation for data with shape: {data.shape}\")\n",
    "            \n",
    "        validation_results = {}\n",
    "            \n",
    "        validation_results = {\n",
    "            'column_presence': self.validate_column_presence(data, schema),\n",
    "            'data_types': self.validate_data_types(data, schema),\n",
    "            'missing_values': self.check_missing_values(data),\n",
    "        }\n",
    "            \n",
    "        is_valid = all(validation_results.values())\n",
    "\n",
    "        create_directories([os.path.dirname(self.config.status_file)])\n",
    "\n",
    "        for check, result in validation_results.items():\n",
    "            logger.info(f\"{check}: {'PASSED' if result else 'FAILED'}\")\n",
    "\n",
    "        logger.info(f\"Overall validation status: {'PASSED' if is_valid else 'FAILED'}\")\n",
    "\n",
    "        with open(self.config.status_file, 'w') as f:\n",
    "            json.dump({\"validation_status\": is_valid}, f, indent=4)\n",
    "\n",
    "        return is_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e395b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "from WattPredictor.config.data_config import DataTransformationConfig\n",
    "from WattPredictor.utils.feature import feature_store_instance\n",
    "from WattPredictor.utils.helpers import create_directories, save_bin\n",
    "from WattPredictor.utils.exception import CustomException\n",
    "from WattPredictor.utils.logging import logger\n",
    "\n",
    "\n",
    "class Engineering:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.feature_store =feature_store_instance()\n",
    "    def check_status(self):\n",
    "        try:\n",
    "            with open(self.config.status_file, 'r') as f:\n",
    "                status_data = json.load(f)\n",
    "            return status_data.get(\"validation_status\", False)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Validation status check failed: {e}\")\n",
    "            return False\n",
    "\n",
    "    def basic_preprocessing(self) -> pd.DataFrame:\n",
    "        try:\n",
    "            fg = self.feature_store.feature_store.get_feature_group(name=\"elec_wx_demand\", version=1)\n",
    "            df = fg.read()\n",
    "            le = LabelEncoder()\n",
    "            df['sub_region_code'] = le.fit_transform(df['subba'])\n",
    "            df.rename(columns={'subba': 'sub_region', 'value': 'demand'}, inplace=True)\n",
    "            df = df[['date_str','date', 'sub_region_code', 'demand', 'temperature_2m']]\n",
    "\n",
    "            create_directories([os.path.dirname(self.config.label_encoder)])\n",
    "            save_bin(le, self.config.label_encoder)\n",
    "            self.feature_store.upload_file_safely(self.config.label_encoder, \"label_encoder.pkl\")\n",
    "\n",
    "            logger.info(\"Label encoding and preprocessing complete.\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def feature_engineering(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        try:\n",
    "            df['date'] = pd.to_datetime(df['date'], utc=True)\n",
    "            \n",
    "            df['hour'] = df['date'].dt.hour\n",
    "            df['day_of_week'] = df['date'].dt.dayofweek\n",
    "            df['month'] = df['date'].dt.month\n",
    "            df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "            holidays = calendar().holidays(start=df['date'].min(), end=df['date'].max())\n",
    "            df['is_holiday'] = df['date'].isin(holidays).astype(int)\n",
    "\n",
    "            \n",
    "            self.feature_store.create_feature_group(\n",
    "                name=\"elec_wx_features\",\n",
    "                df=df,\n",
    "                primary_key=[\"date_str\",\"sub_region_code\"],\n",
    "                event_time=\"date\",\n",
    "                description=\"Engineered electricity demand features\",\n",
    "                online_enabled=True\n",
    "            )\n",
    "\n",
    "            logger.info(\"Feature group created and feature engineering complete.\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def transform(self):\n",
    "        if not self.check_status():\n",
    "            raise CustomException(\"Validation failed. Aborting transformation.\", sys)\n",
    "        try:\n",
    "            df = self.feature_engineering(self.basic_preprocessing())\n",
    "            df.sort_values(\"date\", inplace=True)\n",
    "\n",
    "            self.feature_store.create_feature_view(\n",
    "                name=\"elec_wx_features_view\",\n",
    "                feature_group_name=\"elec_wx_features\",\n",
    "                features=[\n",
    "                    \"date\", \"sub_region_code\", \"demand\", \"temperature_2m\",\n",
    "                    \"hour\", \"day_of_week\", \"month\", \"is_weekend\", \"is_holiday\"\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            self.feature_store.save_training_dataset(\n",
    "                feature_view_name=\"elec_wx_features_view\",\n",
    "                version_description=\"initial training dataset with all features\",\n",
    "                output_format=\"csv\"\n",
    "            )\n",
    "\n",
    "            logger.info(\"Feature view + training dataset saved successfully.\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10b5bc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-16 12:03:34,492: INFO: helpers: yaml file: config_file\\config.yaml loaded successfully]\n",
      "[2025-07-16 12:03:34,509: INFO: helpers: yaml file: config_file\\params.yaml loaded successfully]\n",
      "[2025-07-16 12:03:34,533: INFO: helpers: yaml file: config_file\\schema.yaml loaded successfully]\n",
      "[2025-07-16 12:03:34,534: INFO: helpers: created directory at: artifacts]\n",
      "[2025-07-16 12:03:34,536: INFO: helpers: created directory at: data]\n",
      "[2025-07-16 12:03:34,543: INFO: helpers: yaml file: config_file\\config.yaml loaded successfully]\n",
      "[2025-07-16 12:03:34,546: INFO: helpers: yaml file: config_file\\params.yaml loaded successfully]\n",
      "[2025-07-16 12:03:34,546: INFO: helpers: yaml file: config_file\\schema.yaml loaded successfully]\n",
      "[2025-07-16 12:03:34,546: INFO: external: Initializing external client]\n",
      "[2025-07-16 12:03:34,558: INFO: external: Base URL: https://c.app.hopsworks.ai:443]\n",
      "[2025-07-16 12:03:41,448: INFO: python: Python Engine initialized.]\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1240214\n",
      "[2025-07-16 12:03:44,243: INFO: feature_store: Connected to Hopsworks Feature Store: WattPredictor]\n",
      "[2025-07-16 12:03:44,275: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-01.json]\n",
      "[2025-07-16 12:03:44,321: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-02.json]\n",
      "[2025-07-16 12:03:44,359: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-03.json]\n",
      "[2025-07-16 12:03:44,400: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-04.json]\n",
      "[2025-07-16 12:03:44,437: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-05.json]\n",
      "[2025-07-16 12:03:44,476: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-06.json]\n",
      "[2025-07-16 12:03:44,512: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-07.json]\n",
      "[2025-07-16 12:03:44,562: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-08.json]\n",
      "[2025-07-16 12:03:44,609: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-09.json]\n",
      "[2025-07-16 12:03:44,661: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-10.json]\n",
      "[2025-07-16 12:03:44,690: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-11.json]\n",
      "[2025-07-16 12:03:44,732: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-12.json]\n",
      "[2025-07-16 12:03:44,791: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-13.json]\n",
      "[2025-07-16 12:03:44,832: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-14.json]\n",
      "[2025-07-16 12:03:44,864: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-15.json]\n",
      "[2025-07-16 12:03:44,894: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-16.json]\n",
      "[2025-07-16 12:03:44,926: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-17.json]\n",
      "[2025-07-16 12:03:44,958: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-18.json]\n",
      "[2025-07-16 12:03:44,988: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-19.json]\n",
      "[2025-07-16 12:03:45,019: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-20.json]\n",
      "[2025-07-16 12:03:45,048: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-21.json]\n",
      "[2025-07-16 12:03:45,077: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-22.json]\n",
      "[2025-07-16 12:03:45,097: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-23.json]\n",
      "[2025-07-16 12:03:45,133: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-24.json]\n",
      "[2025-07-16 12:03:45,164: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-25.json]\n",
      "[2025-07-16 12:03:45,203: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-26.json]\n",
      "[2025-07-16 12:03:45,235: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-27.json]\n",
      "[2025-07-16 12:03:45,265: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-28.json]\n",
      "[2025-07-16 12:03:45,297: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-29.json]\n",
      "[2025-07-16 12:03:45,329: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-30.json]\n",
      "[2025-07-16 12:03:45,366: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-01-31.json]\n",
      "[2025-07-16 12:03:45,405: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-01.json]\n",
      "[2025-07-16 12:03:45,427: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-02.json]\n",
      "[2025-07-16 12:03:45,464: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-03.json]\n",
      "[2025-07-16 12:03:45,497: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-04.json]\n",
      "[2025-07-16 12:03:45,529: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-05.json]\n",
      "[2025-07-16 12:03:45,559: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-06.json]\n",
      "[2025-07-16 12:03:45,581: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-07.json]\n",
      "[2025-07-16 12:03:45,621: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-08.json]\n",
      "[2025-07-16 12:03:45,651: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-09.json]\n",
      "[2025-07-16 12:03:45,681: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-10.json]\n",
      "[2025-07-16 12:03:45,714: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-11.json]\n",
      "[2025-07-16 12:03:45,746: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-12.json]\n",
      "[2025-07-16 12:03:45,781: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-13.json]\n",
      "[2025-07-16 12:03:45,810: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-14.json]\n",
      "[2025-07-16 12:03:45,848: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-15.json]\n",
      "[2025-07-16 12:03:45,881: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-16.json]\n",
      "[2025-07-16 12:03:45,927: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-17.json]\n",
      "[2025-07-16 12:03:45,959: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-18.json]\n",
      "[2025-07-16 12:03:45,981: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-19.json]\n",
      "[2025-07-16 12:03:46,015: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-20.json]\n",
      "[2025-07-16 12:03:46,047: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-21.json]\n",
      "[2025-07-16 12:03:46,082: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-22.json]\n",
      "[2025-07-16 12:03:46,116: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-23.json]\n",
      "[2025-07-16 12:03:46,147: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-24.json]\n",
      "[2025-07-16 12:03:46,199: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-25.json]\n",
      "[2025-07-16 12:03:46,246: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-26.json]\n",
      "[2025-07-16 12:03:46,280: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-27.json]\n",
      "[2025-07-16 12:03:46,309: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-02-28.json]\n",
      "[2025-07-16 12:03:46,346: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-01.json]\n",
      "[2025-07-16 12:03:46,371: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-02.json]\n",
      "[2025-07-16 12:03:46,398: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-03.json]\n",
      "[2025-07-16 12:03:46,427: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-04.json]\n",
      "[2025-07-16 12:03:46,448: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-05.json]\n",
      "[2025-07-16 12:03:46,485: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-06.json]\n",
      "[2025-07-16 12:03:46,515: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-07.json]\n",
      "[2025-07-16 12:03:46,548: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-08.json]\n",
      "[2025-07-16 12:03:46,580: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-09.json]\n",
      "[2025-07-16 12:03:46,606: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-10.json]\n",
      "[2025-07-16 12:03:46,625: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-11.json]\n",
      "[2025-07-16 12:03:46,711: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-12.json]\n",
      "[2025-07-16 12:03:46,741: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-13.json]\n",
      "[2025-07-16 12:03:46,783: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-14.json]\n",
      "[2025-07-16 12:03:46,812: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-15.json]\n",
      "[2025-07-16 12:03:46,842: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-16.json]\n",
      "[2025-07-16 12:03:46,859: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-17.json]\n",
      "[2025-07-16 12:03:46,898: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-18.json]\n",
      "[2025-07-16 12:03:46,929: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-19.json]\n",
      "[2025-07-16 12:03:46,964: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-20.json]\n",
      "[2025-07-16 12:03:46,999: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-21.json]\n",
      "[2025-07-16 12:03:47,030: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-22.json]\n",
      "[2025-07-16 12:03:47,056: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-23.json]\n",
      "[2025-07-16 12:03:47,083: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-24.json]\n",
      "[2025-07-16 12:03:47,107: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-25.json]\n",
      "[2025-07-16 12:03:47,131: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-26.json]\n",
      "[2025-07-16 12:03:47,167: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-27.json]\n",
      "[2025-07-16 12:03:47,195: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-28.json]\n",
      "[2025-07-16 12:03:47,213: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-29.json]\n",
      "[2025-07-16 12:03:47,249: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-30.json]\n",
      "[2025-07-16 12:03:47,276: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-03-31.json]\n",
      "[2025-07-16 12:03:47,301: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-01.json]\n",
      "[2025-07-16 12:03:47,331: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-02.json]\n",
      "[2025-07-16 12:03:47,359: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-03.json]\n",
      "[2025-07-16 12:03:47,381: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-04.json]\n",
      "[2025-07-16 12:03:47,421: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-05.json]\n",
      "[2025-07-16 12:03:47,442: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-06.json]\n",
      "[2025-07-16 12:03:47,482: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-07.json]\n",
      "[2025-07-16 12:03:47,509: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-08.json]\n",
      "[2025-07-16 12:03:47,548: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-09.json]\n",
      "[2025-07-16 12:03:47,571: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-10.json]\n",
      "[2025-07-16 12:03:47,597: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-11.json]\n",
      "[2025-07-16 12:03:47,628: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-12.json]\n",
      "[2025-07-16 12:03:47,650: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-13.json]\n",
      "[2025-07-16 12:03:47,679: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-14.json]\n",
      "[2025-07-16 12:03:47,696: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-15.json]\n",
      "[2025-07-16 12:03:47,731: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-16.json]\n",
      "[2025-07-16 12:03:47,764: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-17.json]\n",
      "[2025-07-16 12:03:47,792: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-18.json]\n",
      "[2025-07-16 12:03:47,809: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-19.json]\n",
      "[2025-07-16 12:03:47,850: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-20.json]\n",
      "[2025-07-16 12:03:47,882: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-21.json]\n",
      "[2025-07-16 12:03:47,893: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-22.json]\n",
      "[2025-07-16 12:03:47,934: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-23.json]\n",
      "[2025-07-16 12:03:47,966: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-24.json]\n",
      "[2025-07-16 12:03:47,996: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-25.json]\n",
      "[2025-07-16 12:03:48,016: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-26.json]\n",
      "[2025-07-16 12:03:48,042: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-27.json]\n",
      "[2025-07-16 12:03:48,075: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-28.json]\n",
      "[2025-07-16 12:03:48,099: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-29.json]\n",
      "[2025-07-16 12:03:48,131: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-04-30.json]\n",
      "[2025-07-16 12:03:48,159: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-01.json]\n",
      "[2025-07-16 12:03:48,186: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-02.json]\n",
      "[2025-07-16 12:03:48,234: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-03.json]\n",
      "[2025-07-16 12:03:48,266: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-04.json]\n",
      "[2025-07-16 12:03:48,296: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-05.json]\n",
      "[2025-07-16 12:03:48,332: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-06.json]\n",
      "[2025-07-16 12:03:48,362: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-07.json]\n",
      "[2025-07-16 12:03:48,375: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-08.json]\n",
      "[2025-07-16 12:03:48,408: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-09.json]\n",
      "[2025-07-16 12:03:48,435: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-10.json]\n",
      "[2025-07-16 12:03:48,467: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-11.json]\n",
      "[2025-07-16 12:03:48,492: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-12.json]\n",
      "[2025-07-16 12:03:48,515: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-13.json]\n",
      "[2025-07-16 12:03:48,548: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-14.json]\n",
      "[2025-07-16 12:03:48,575: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-15.json]\n",
      "[2025-07-16 12:03:48,601: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-16.json]\n",
      "[2025-07-16 12:03:48,626: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-17.json]\n",
      "[2025-07-16 12:03:48,676: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-18.json]\n",
      "[2025-07-16 12:03:48,743: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-19.json]\n",
      "[2025-07-16 12:03:48,759: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-20.json]\n",
      "[2025-07-16 12:03:48,780: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-21.json]\n",
      "[2025-07-16 12:03:48,813: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-22.json]\n",
      "[2025-07-16 12:03:48,842: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-23.json]\n",
      "[2025-07-16 12:03:48,865: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-24.json]\n",
      "[2025-07-16 12:03:48,904: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-25.json]\n",
      "[2025-07-16 12:03:48,928: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-26.json]\n",
      "[2025-07-16 12:03:48,952: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-27.json]\n",
      "[2025-07-16 12:03:48,975: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-28.json]\n",
      "[2025-07-16 12:03:49,000: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-29.json]\n",
      "[2025-07-16 12:03:49,026: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-30.json]\n",
      "[2025-07-16 12:03:49,049: INFO: helpers: json file loaded succesfully from: data\\raw\\elec_data\\hourly_demand_2025-05-31.json]\n",
      "[2025-07-16 12:03:50,063: INFO: feature_store: Feature Group 'elec_wx_demand' v1 exists. Deleting it.]\n",
      "[2025-07-16 12:03:50,063: WARNING: warnings: JobWarning: All jobs associated to feature group `elec_wx_demand`, version `1` will be removed.\n",
      "]\n",
      "[2025-07-16 12:03:51,513: INFO: feature_store: Creating Feature Group 'elec_wx_demand' v1.]\n",
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/1240214/fs/1223749/fg/1495483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 41184/41184 | Elapsed Time: 00:17 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: elec_wx_demand_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1240214/jobs/named/elec_wx_demand_1_offline_fg_materialization/executions\n",
      "[2025-07-16 12:04:25,075: INFO: feature_store: Feature Group 'elec_wx_demand' v1 created and data inserted.]\n",
      "[2025-07-16 12:04:25,080: INFO: helpers: created directory at: data\\processed]\n",
      "[2025-07-16 12:04:25,462: INFO: ingestion: Saved combined dataset to data\\processed\\elec_wx_demand.csv]\n",
      "[2025-07-16 12:04:25,462: INFO: helpers: created directory at: artifacts/data_validation]\n",
      "[2025-07-16 12:04:25,578: INFO: validation: Starting validation for data with shape: (41184, 13)]\n",
      "[2025-07-16 12:04:25,598: INFO: helpers: created directory at: artifacts/data_validation]\n",
      "[2025-07-16 12:04:25,598: INFO: validation: column_presence: PASSED]\n",
      "[2025-07-16 12:04:25,598: INFO: validation: data_types: PASSED]\n",
      "[2025-07-16 12:04:25,598: INFO: validation: missing_values: PASSED]\n",
      "[2025-07-16 12:04:25,604: INFO: validation: Overall validation status: PASSED]\n",
      "[2025-07-16 12:04:25,607: INFO: helpers: created directory at: artifacts/data_transformation]\n",
      "[2025-07-16 12:04:30,016: ERROR: arrow_flight_client: [Errno 2] Opening HDFS file '/apps/hive/warehouse/wattpredictor_featurestore.db/elec_wx_demand_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\n",
      "  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n",
      "    result = func(instance, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 196, in do_get\n",
      "    return self._read_query(context, path, command)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n",
      "    return func(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n",
      "    result_batches = self.hudi_query_engine.read_query(query_obj)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_query_engine.py\", line 63, in read_query\n",
      "    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 55, in get_featuregroup_parquet_paths\n",
      "    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 185, in _get_partition_keys_from_metadata\n",
      "    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \"rt\") as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1139, in open\n",
      "    self.open(\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1151, in open\n",
      "    f = self._open(\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 22, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 178, in _open\n",
      "    stream = method(path, **_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/_fs.pyx\", line 789, in pyarrow._fs.FileSystem.open_input_file\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/wattpredictor_featurestore.db/elec_wx_demand_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n",
      "  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n",
      "    raise FlyingDuckException(str(e)) from e\n",
      "utils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file '/apps/hive/warehouse/wattpredictor_featurestore.db/elec_wx_demand_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      ". gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/wattpredictor_featurestore.db/elec_wx_demand_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 63, in read_query\\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 55, in get_featuregroup_parquet_paths\\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 185, in _get_partition_keys_from_metadata\\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \\\"rt\\\") as f:\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1139, in open\\n    self.open(\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1151, in open\\n    f = self._open(\\n        ^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 22, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 178, in _open\\n    stream = method(path, **_kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"pyarrow/_fs.pyx\\\", line 789, in pyarrow._fs.FileSystem.open_input_file\\n  File \\\"pyarrow/error.pxi\\\", line 155, in pyarrow.lib.pyarrow_internal_check_status\\n  File \\\"pyarrow/error.pxi\\\", line 92, in pyarrow.lib.check_status\\nFileNotFoundError: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/wattpredictor_featurestore.db/elec_wx_demand_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/wattpredictor_featurestore.db/elec_wx_demand_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\", grpc_status:2, created_time:\"2025-07-16T06:34:29.9381307+00:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal]\n",
      "Traceback (most recent call last):\n",
      "  File \"f:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 394, in afs_error_handler_wrapper\n",
      "    return func(instance, *args, **kw)\n",
      "  File \"f:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 459, in read_query\n",
      "    return self._get_dataset(\n",
      "  File \"f:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\retrying.py\", line 55, in wrapped_f\n",
      "    return Retrying(*dargs, **dkw).call(f, *args, **kw)\n",
      "  File \"f:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\retrying.py\", line 265, in call\n",
      "    return attempt.get(self._wrap_exception)\n",
      "  File \"f:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\retrying.py\", line 312, in get\n",
      "    raise exc.with_traceback(tb)\n",
      "  File \"f:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\retrying.py\", line 259, in call\n",
      "    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)\n",
      "  File \"f:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py\", line 445, in _get_dataset\n",
      "    reader = self._connection.do_get(info.endpoints[0].ticket, options)\n",
      "  File \"pyarrow\\\\_flight.pyx\", line 1708, in pyarrow._flight.FlightClient.do_get\n",
      "  File \"pyarrow\\\\_flight.pyx\", line 58, in pyarrow._flight.check_flight_status\n",
      "pyarrow._flight.FlightServerError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/wattpredictor_featurestore.db/elec_wx_demand_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\n",
      "  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n",
      "    result = func(instance, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 196, in do_get\n",
      "    return self._read_query(context, path, command)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n",
      "    return func(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n",
      "    result_batches = self.hudi_query_engine.read_query(query_obj)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_query_engine.py\", line 63, in read_query\n",
      "    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 55, in get_featuregroup_parquet_paths\n",
      "    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 185, in _get_partition_keys_from_metadata\n",
      "    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \"rt\") as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1139, in open\n",
      "    self.open(\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1151, in open\n",
      "    f = self._open(\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 22, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 178, in _open\n",
      "    stream = method(path, **_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/_fs.pyx\", line 789, in pyarrow._fs.FileSystem.open_input_file\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/wattpredictor_featurestore.db/elec_wx_demand_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n",
      "  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n",
      "    raise FlyingDuckException(str(e)) from e\n",
      "utils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file '/apps/hive/warehouse/wattpredictor_featurestore.db/elec_wx_demand_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      ". gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/wattpredictor_featurestore.db/elec_wx_demand_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 63, in read_query\\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 55, in get_featuregroup_parquet_paths\\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 185, in _get_partition_keys_from_metadata\\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \\\"rt\\\") as f:\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1139, in open\\n    self.open(\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1151, in open\\n    f = self._open(\\n        ^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 22, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 178, in _open\\n    stream = method(path, **_kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"pyarrow/_fs.pyx\\\", line 789, in pyarrow._fs.FileSystem.open_input_file\\n  File \\\"pyarrow/error.pxi\\\", line 155, in pyarrow.lib.pyarrow_internal_check_status\\n  File \\\"pyarrow/error.pxi\\\", line 92, in pyarrow.lib.check_status\\nFileNotFoundError: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/wattpredictor_featurestore.db/elec_wx_demand_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/wattpredictor_featurestore.db/elec_wx_demand_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\", grpc_status:2, created_time:\"2025-07-16T06:34:29.9381307+00:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "Error: Reading data from Hopsworks, using Hopsworks Feature Query Service           \n"
     ]
    },
    {
     "ename": "CustomException",
     "evalue": "Exception in F:\\WattPredictor\\src\\WattPredictor\\components\\features\\engineering.py, line 80: Exception in F:\\WattPredictor\\src\\WattPredictor\\components\\features\\engineering.py, line 34: Could not read data using Hopsworks Query Service.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFlightServerError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mf:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py:394\u001b[0m, in \u001b[0;36mArrowFlightClient._handle_afs_exception.<locals>.decorator.<locals>.afs_error_handler_wrapper\u001b[1;34m(instance, *args, **kw)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 394\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(instance, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mf:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py:459\u001b[0m, in \u001b[0;36mArrowFlightClient.read_query\u001b[1;34m(self, query_object, arrow_flight_config, dataframe_type)\u001b[0m\n\u001b[0;32m    458\u001b[0m descriptor \u001b[38;5;241m=\u001b[39m pyarrow\u001b[38;5;241m.\u001b[39mflight\u001b[38;5;241m.\u001b[39mFlightDescriptor\u001b[38;5;241m.\u001b[39mfor_command(query_encoded)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescriptor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrow_flight_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimeout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marrow_flight_config\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataframe_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\retrying.py:55\u001b[0m, in \u001b[0;36mretry.<locals>.wrap.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Retrying(\u001b[38;5;241m*\u001b[39mdargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdkw)\u001b[38;5;241m.\u001b[39mcall(f, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mf:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\retrying.py:265\u001b[0m, in \u001b[0;36mRetrying.call\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_reject(attempt):\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mattempt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39mwarn(attempt)\n",
      "File \u001b[1;32mf:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\retrying.py:312\u001b[0m, in \u001b[0;36mAttempt.get\u001b[1;34m(self, wrap_exception)\u001b[0m\n\u001b[0;32m    311\u001b[0m         exc_type, exc, tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m--> 312\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mf:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\retrying.py:259\u001b[0m, in \u001b[0;36mRetrying.call\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 259\u001b[0m     attempt \u001b[38;5;241m=\u001b[39m Attempt(fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), attempt_number, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[1;32mf:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py:445\u001b[0m, in \u001b[0;36mArrowFlightClient._get_dataset\u001b[1;34m(self, descriptor, timeout, dataframe_type)\u001b[0m\n\u001b[0;32m    442\u001b[0m options \u001b[38;5;241m=\u001b[39m pyarrow\u001b[38;5;241m.\u001b[39mflight\u001b[38;5;241m.\u001b[39mFlightCallOptions(\n\u001b[0;32m    443\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout, headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_certificates_headers()\n\u001b[0;32m    444\u001b[0m )\n\u001b[1;32m--> 445\u001b[0m reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoints\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mticket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset fetched. Converting to dataframe \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataframe_type)\n",
      "File \u001b[1;32mf:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\pyarrow\\_flight.pyx:1708\u001b[0m, in \u001b[0;36mpyarrow._flight.FlightClient.do_get\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mf:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\pyarrow\\_flight.pyx:58\u001b[0m, in \u001b[0;36mpyarrow._flight.check_flight_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFlightServerError\u001b[0m: [Errno 2] Opening HDFS file '/apps/hive/warehouse/wattpredictor_featurestore.db/elec_wx_demand_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\n  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n    result = func(instance, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/server.py\", line 196, in do_get\n    return self._read_query(context, path, command)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n    return func(instance, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n    result_batches = self.hudi_query_engine.read_query(query_obj)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/hudi_query_engine.py\", line 63, in read_query\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 55, in get_featuregroup_parquet_paths\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 185, in _get_partition_keys_from_metadata\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \"rt\") as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1139, in open\n    self.open(\n  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1151, in open\n    f = self._open(\n        ^^^^^^^^^^^\n  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 22, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 178, in _open\n    stream = method(path, **_kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pyarrow/_fs.pyx\", line 789, in pyarrow._fs.FileSystem.open_input_file\n  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\nFileNotFoundError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/wattpredictor_featurestore.db/elec_wx_demand_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n    raise FlyingDuckException(str(e)) from e\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file '/apps/hive/warehouse/wattpredictor_featurestore.db/elec_wx_demand_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n. gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/wattpredictor_featurestore.db/elec_wx_demand_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 63, in read_query\\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 55, in get_featuregroup_parquet_paths\\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 185, in _get_partition_keys_from_metadata\\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \\\"rt\\\") as f:\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1139, in open\\n    self.open(\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1151, in open\\n    f = self._open(\\n        ^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 22, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 178, in _open\\n    stream = method(path, **_kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"pyarrow/_fs.pyx\\\", line 789, in pyarrow._fs.FileSystem.open_input_file\\n  File \\\"pyarrow/error.pxi\\\", line 155, in pyarrow.lib.pyarrow_internal_check_status\\n  File \\\"pyarrow/error.pxi\\\", line 92, in pyarrow.lib.check_status\\nFileNotFoundError: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/wattpredictor_featurestore.db/elec_wx_demand_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/wattpredictor_featurestore.db/elec_wx_demand_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\", grpc_status:2, created_time:\"2025-07-16T06:34:29.9381307+00:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mFeatureStoreException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[1;32mF:\\WattPredictor\\src\\WattPredictor\\components\\features\\engineering.py:34\u001b[0m, in \u001b[0;36mEngineering.basic_preprocessing\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     33\u001b[0m fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_store\u001b[38;5;241m.\u001b[39mfeature_store\u001b[38;5;241m.\u001b[39mget_feature_group(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melec_wx_demand\u001b[39m\u001b[38;5;124m\"\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m le \u001b[38;5;241m=\u001b[39m LabelEncoder()\n",
      "File \u001b[1;32mf:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\hsfs\\feature_group.py:2558\u001b[0m, in \u001b[0;36mFeatureGroup.read\u001b[1;34m(self, wallclock_time, online, dataframe_type, read_options)\u001b[0m\n\u001b[0;32m   2557\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2558\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2559\u001b[0m \u001b[43m        \u001b[49m\u001b[43monline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataframe_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mread_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2562\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\hsfs\\constructor\\query.py:309\u001b[0m, in \u001b[0;36mQuery.read\u001b[1;34m(self, online, dataframe_type, read_options)\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    306\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas types casting only supported for feature_group.read()/query.select_all()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         )\n\u001b[1;32m--> 309\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_store_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43monline_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataframe_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\hsfs\\engine\\python.py:146\u001b[0m, in \u001b[0;36mEngine.sql\u001b[1;34m(self, sql_query, feature_store, online_conn, dataframe_type, read_options, schema)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m online_conn:\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sql_offline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataframe_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrow_flight_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marrow_flight_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mread_options\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mf:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\hsfs\\engine\\python.py:189\u001b[0m, in \u001b[0;36mEngine._sql_offline\u001b[1;34m(self, sql_query, dataframe_type, schema, arrow_flight_config)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhsfs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m arrow_flight_client\n\u001b[1;32m--> 189\u001b[0m     result_df \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_loading_animation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReading data from Hopsworks, using Hopsworks Feature Query Service\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrow_flight_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrow_flight_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataframe_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mf:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\hopsworks_common\\util.py:373\u001b[0m, in \u001b[0;36mrun_with_loading_animation\u001b[1;34m(message, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 373\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    374\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32mf:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\hsfs\\core\\arrow_flight_client.py:412\u001b[0m, in \u001b[0;36mArrowFlightClient._handle_afs_exception.<locals>.decorator.<locals>.afs_error_handler_wrapper\u001b[1;34m(instance, *args, **kw)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 412\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FeatureStoreException(user_message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mFeatureStoreException\u001b[0m: Could not read data using Hopsworks Query Service.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCustomException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mF:\\WattPredictor\\src\\WattPredictor\\components\\features\\engineering.py:80\u001b[0m, in \u001b[0;36mEngineering.transform\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 80\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_engineering(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_preprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     81\u001b[0m     df\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mF:\\WattPredictor\\src\\WattPredictor\\components\\features\\engineering.py:47\u001b[0m, in \u001b[0;36mEngineering.basic_preprocessing\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CustomException(e, sys)\n",
      "\u001b[1;31mCustomException\u001b[0m: Exception in F:\\WattPredictor\\src\\WattPredictor\\components\\features\\engineering.py, line 34: Could not read data using Hopsworks Query Service.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCustomException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m engineering_config \u001b[38;5;241m=\u001b[39m data_config\u001b[38;5;241m.\u001b[39mget_data_transformation_config()\n\u001b[0;32m     18\u001b[0m transformation \u001b[38;5;241m=\u001b[39m Engineering(config\u001b[38;5;241m=\u001b[39mengineering_config)\n\u001b[1;32m---> 19\u001b[0m transformed_data \u001b[38;5;241m=\u001b[39m \u001b[43mtransformation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transformed_data\n",
      "File \u001b[1;32mF:\\WattPredictor\\src\\WattPredictor\\components\\features\\engineering.py:101\u001b[0m, in \u001b[0;36mEngineering.transform\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CustomException(e, sys)\n",
      "\u001b[1;31mCustomException\u001b[0m: Exception in F:\\WattPredictor\\src\\WattPredictor\\components\\features\\engineering.py, line 80: Exception in F:\\WattPredictor\\src\\WattPredictor\\components\\features\\engineering.py, line 34: Could not read data using Hopsworks Query Service."
     ]
    }
   ],
   "source": [
    "from WattPredictor.config.data_config import DataConfigurationManager\n",
    "from WattPredictor.components.data.ingestion import Ingestion\n",
    "from WattPredictor.components.data.validation import Validation\n",
    "from WattPredictor.components.features.engineering import Engineering\n",
    "\n",
    "\n",
    "data_config = DataConfigurationManager()\n",
    "\n",
    "ingestion_config = data_config.get_data_ingestion_config()\n",
    "ingestion = Ingestion(config=ingestion_config)\n",
    "raw_data = ingestion.download()\n",
    "\n",
    "data_validation_config = data_config.get_data_validation_config()\n",
    "data_validation = Validation(data_validation_config)\n",
    "data_validation.validator()\n",
    "\n",
    "engineering_config = data_config.get_data_transformation_config()\n",
    "transformation = Engineering(config=engineering_config)\n",
    "transformed_data = transformation.transform()\n",
    "\n",
    "transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6360ef49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WattPredictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
