{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4d21fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a82b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b945b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.getcwd(), \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db4d2e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from electron.utils.helpers import *\n",
    "from electron.utils.exception import *\n",
    "from electron.constants import *\n",
    "from electron import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e87464eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_file: Path\n",
    "    status_file: str\n",
    "    label_encoder: Path\n",
    "    preprocessor: Path\n",
    "    x_transform: Path\n",
    "    y_transform: Path\n",
    "    train_features: Path\n",
    "    test_features: Path\n",
    "    train_target: Path\n",
    "    test_target: Path\n",
    "    input_seq_len: int\n",
    "    step_size: int\n",
    "    cutoff_date: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04e5eb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_PATH,\n",
    "                       params_filepath=PARAMS_PATH,\n",
    "                       schema_filepath=SCHEMA_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        schema = self.schema\n",
    "        params = self.params.transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            data_file=Path(config.data_file),\n",
    "            status_file=Path(config.status_file),\n",
    "            label_encoder=Path(config.label_encoder),\n",
    "            preprocessor=Path(config.preprocessor),\n",
    "            x_transform=Path(config.x_transform),\n",
    "            y_transform=Path(config.y_transform),\n",
    "            train_features=Path(config.train_features),\n",
    "            test_features=Path(config.test_features),\n",
    "            train_target=Path(config.train_target),\n",
    "            test_target=Path(config.test_target),\n",
    "            input_seq_len=params.input_seq_len,\n",
    "            step_size=params.step_size,\n",
    "            cutoff_date=params.cutoff_date\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42bdf4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def check_status(self):\n",
    "        try:\n",
    "            with open(self.config.status_file, 'r') as f:\n",
    "                status_data = json.load(f)\n",
    "            validation_status = status_data.get(\"Validation_status\", False)\n",
    "            logger.info(f\"Data validation status: {validation_status}\")\n",
    "            return validation_status\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"Invalid JSON in status file: {e}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading validation status: {e}\")\n",
    "            return False\n",
    "\n",
    "\n",
    "    def basic_preprocessing(self) -> pd.DataFrame:\n",
    "        try:\n",
    "            df = pd.read_csv(self.config.data_file)\n",
    "            df = df[['period', 'subba', 'value', 'temperature_2m']]\n",
    "            le = LabelEncoder()\n",
    "            df['sub_region_code'] = le.fit_transform(df['subba'])\n",
    "\n",
    "            df.rename(columns={\n",
    "                'period': 'date',\n",
    "                'subba': 'sub_region',\n",
    "                'value': 'demand'\n",
    "            }, inplace=True)\n",
    "\n",
    "            df = df[['date', 'sub_region_code', 'demand', 'temperature_2m']]\n",
    "\n",
    "            create_directories([os.path.dirname(self.config.label_encoder)])\n",
    "            save_bin(le, self.config.label_encoder)\n",
    "\n",
    "            logger.info(\"Basic preprocessing completed.\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def feature_engineering(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        try:\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce', utc=True)\n",
    "\n",
    "            df['hour'] = df['date'].dt.hour\n",
    "            df['day_of_week'] = df['date'].dt.dayofweek\n",
    "            df['month'] = df['date'].dt.month\n",
    "            df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "            holidays = calendar().holidays(start=df['date'].min(), end=df['date'].max())\n",
    "            df['is_holiday'] = df['date'].isin(holidays).astype(int)\n",
    "\n",
    "            logger.info(\"Feature engineering completed.\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def train_test_splitting(self) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        try:\n",
    "            df = self.feature_engineering(self.basic_preprocessing())\n",
    "            df.sort_values(\"date\", inplace=True)\n",
    "\n",
    "            cutoff = pd.to_datetime(self.config.cutoff_date, utc=True)\n",
    "\n",
    "            train_df = df[df['date'] < cutoff].reset_index(drop=True)\n",
    "            test_df = df[df['date'] >= cutoff].reset_index(drop=True)\n",
    "\n",
    "            logger.info(f\"Train size: {train_df.shape}, Test size: {test_df.shape}\")\n",
    "            return train_df, test_df\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "    def _get_cutoff_indices(self, df: pd.DataFrame, input_seq_len: int, step_size: int):\n",
    "        stop = len(df) - input_seq_len - 1\n",
    "        return [(i, i + input_seq_len, i + input_seq_len + 1) for i in range(0, stop, step_size)]\n",
    "        \n",
    "    def transform_ts_data_into_features_and_target(self, ts_data: pd.DataFrame) -> tuple[pd.DataFrame, pd.Series]:\n",
    "            \n",
    "            assert set(['date', 'demand', 'sub_region_code', 'temperature_2m']).issubset(ts_data.columns)\n",
    "\n",
    "            region_codes = ts_data['sub_region_code'].unique()\n",
    "            features = pd.DataFrame()\n",
    "            targets = pd.DataFrame()\n",
    "\n",
    "            input_seq_len = self.config.input_seq_len\n",
    "            step_size = self.config.step_size\n",
    "\n",
    "            for code in tqdm.tqdm(region_codes, desc=\"Transforming TS Data\"):\n",
    "                ts_one = ts_data[ts_data['sub_region_code'] == code].sort_values(by='date')\n",
    "                indices = self._get_cutoff_indices(ts_one, input_seq_len, step_size)\n",
    "\n",
    "                x = np.zeros((len(indices), input_seq_len), dtype=np.float64)\n",
    "                y = np.zeros((len(indices)), dtype=np.float64)\n",
    "                date_hours, temps = [], []\n",
    "\n",
    "                for i, (start, mid, end) in enumerate(indices):\n",
    "                    x[i, :] = ts_one.iloc[start:mid]['demand'].values\n",
    "                    y[i] = ts_one.iloc[mid]['demand']\n",
    "                    date_hours.append(ts_one.iloc[mid]['date'])\n",
    "                    temps.append(ts_one.iloc[mid]['temperature_2m'])\n",
    "\n",
    "                features_one = pd.DataFrame(\n",
    "                    x,\n",
    "                    columns=[f'demand_prev_{i+1}_hr' for i in reversed(range(input_seq_len))]\n",
    "                )\n",
    "                features_one['date'] = date_hours\n",
    "                features_one['sub_region_code'] = code\n",
    "                features_one['temperature_2m'] = temps\n",
    "\n",
    "                targets_one = pd.DataFrame(y, columns=['target_demand_next_hour'])\n",
    "\n",
    "                features = pd.concat([features, features_one], ignore_index=True)\n",
    "                targets = pd.concat([targets, targets_one], ignore_index=True)\n",
    "\n",
    "            return features, targets['target_demand_next_hour']\n",
    "\n",
    "    def preprocess_features(self, train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "        try:\n",
    "            validation_status = self.check_status()\n",
    "\n",
    "            if not validation_status:\n",
    "                logger.error(\"Data validation failed. Aborting data transformation.\")\n",
    "                return None\n",
    "\n",
    "            logger.info(\"Validation PASSED. Proceeding with feature transformation...\")\n",
    "\n",
    "            # Transform to supervised format\n",
    "            train_x, train_y = self.transform_ts_data_into_features_and_target(train_df)\n",
    "            test_x, test_y = self.transform_ts_data_into_features_and_target(test_df)\n",
    "\n",
    "            # Drop 'date' before saving as .npy\n",
    "            train_x_npy = train_x.drop(columns=[\"date\"], errors=\"ignore\")\n",
    "            test_x_npy = test_x.drop(columns=[\"date\"], errors=\"ignore\")\n",
    "\n",
    "            # Save numpy arrays\n",
    "            np.save(self.config.x_transform, train_x_npy.values)\n",
    "            np.save(self.config.y_transform, train_y.values)\n",
    "\n",
    "            logger.info(f\"Shapes - Train X: {train_x_npy.shape}, Train Y: {train_y.shape}, test X: {test_x_npy.shape}, Test Y: {test_y.shape}\")\n",
    "\n",
    "            # Save CSVs for inspection\n",
    "            train_x.to_csv(self.config.train_features, index=False)\n",
    "            train_y.to_csv(self.config.train_target, index=False)\n",
    "            test_x.to_csv(self.config.test_features, index=False)\n",
    "            test_y.to_csv(self.config.test_target, index=False)\n",
    "\n",
    "            logger.info(\"Feature transformation and saving completed.\")\n",
    "            return (train_x, train_y), (test_x, test_y)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d60c4364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 10:04:10,198: INFO: helpers: yaml file: config_file\\config.yaml loaded successfully]\n",
      "[2025-07-06 10:04:10,203: INFO: helpers: yaml file: config_file\\params.yaml loaded successfully]\n",
      "[2025-07-06 10:04:10,210: INFO: helpers: yaml file: config_file\\schema.yaml loaded successfully]\n",
      "[2025-07-06 10:04:10,210: INFO: helpers: created directory at: artifacts]\n",
      "[2025-07-06 10:04:10,214: INFO: helpers: created directory at: data_transformation]\n",
      "[2025-07-06 10:04:10,417: INFO: helpers: created directory at: artifacts\\data_transformation]\n",
      "[2025-07-06 10:04:10,417: INFO: helpers: binary file saved at: artifacts\\data_transformation\\label_encoder.pkl]\n",
      "[2025-07-06 10:04:10,417: INFO: 2151164191: Basic preprocessing completed.]\n",
      "[2025-07-06 10:04:10,467: INFO: 2151164191: Feature engineering completed.]\n",
      "[2025-07-06 10:04:10,514: INFO: 2151164191: Train size: (80245, 9), Test size: (20086, 9)]\n",
      "[2025-07-06 10:04:10,519: INFO: 2151164191: Data validation status: True]\n",
      "[2025-07-06 10:04:10,521: INFO: 2151164191: Validation PASSED. Proceeding with feature transformation...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming TS Data: 100%|██████████| 11/11 [00:21<00:00,  1.97s/it]\n",
      "Transforming TS Data: 100%|██████████| 11/11 [00:03<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-06 10:04:37,548: INFO: 2151164191: Shapes - Train X: (72842, 674), Train Y: (72842,), test X: (12683, 674), Test Y: (12683,)]\n",
      "[2025-07-06 10:05:07,204: INFO: 2151164191: Feature transformation and saving completed.]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    train_df, test_df = data_transformation.train_test_splitting()\n",
    "    (train_x, train_y), (test_x, test_y) = data_transformation.preprocess_features(train_df, test_df)\n",
    "\n",
    "except Exception as e:\n",
    "    raise CustomException(str(e), sys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ele",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
