{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4d21fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a82b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b945b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.getcwd(), \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db4d2e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from WattPredictor.utils.helpers import *\n",
    "from WattPredictor.utils.exception import *\n",
    "from WattPredictor.constants import *\n",
    "from WattPredictor import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e87464eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_file: Path\n",
    "    status_file: str\n",
    "    label_encoder: Path\n",
    "    preprocessor: Path\n",
    "    x_transform: Path\n",
    "    y_transform: Path\n",
    "    train_features: Path\n",
    "    test_features: Path\n",
    "    train_target: Path\n",
    "    test_target: Path\n",
    "    input_seq_len: int\n",
    "    step_size: int\n",
    "    cutoff_date: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04e5eb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath=CONFIG_PATH,\n",
    "                       params_filepath=PARAMS_PATH,\n",
    "                       schema_filepath=SCHEMA_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        schema = self.schema\n",
    "        params = self.params.transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            data_file=Path(config.data_file),\n",
    "            status_file=Path(config.status_file),\n",
    "            label_encoder=Path(config.label_encoder),\n",
    "            preprocessor=Path(config.preprocessor),\n",
    "            x_transform=Path(config.x_transform),\n",
    "            y_transform=Path(config.y_transform),\n",
    "            train_features=Path(config.train_features),\n",
    "            test_features=Path(config.test_features),\n",
    "            train_target=Path(config.train_target),\n",
    "            test_target=Path(config.test_target),\n",
    "            input_seq_len=params.input_seq_len,\n",
    "            step_size=params.step_size,\n",
    "            cutoff_date=params.cutoff_date\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42bdf4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def check_status(self):\n",
    "        try:\n",
    "            with open(self.config.status_file, 'r') as f:\n",
    "                status_data = json.load(f)\n",
    "            validation_status = status_data.get(\"validation_status\", False)\n",
    "            logger.info(f\"Data validation status: {validation_status}\")\n",
    "            return validation_status\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"Invalid JSON in status file: {e}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading validation status: {e}\")\n",
    "            return False\n",
    "\n",
    "\n",
    "    def basic_preprocessing(self) -> pd.DataFrame:\n",
    "        try:\n",
    "            df = pd.read_csv(self.config.data_file)\n",
    "            df = df[['period', 'subba', 'value', 'temperature_2m']]\n",
    "            le = LabelEncoder()\n",
    "            df['sub_region_code'] = le.fit_transform(df['subba'])\n",
    "\n",
    "            df.rename(columns={\n",
    "                'period': 'date',\n",
    "                'subba': 'sub_region',\n",
    "                'value': 'demand'\n",
    "            }, inplace=True)\n",
    "\n",
    "            df = df[['date', 'sub_region_code', 'demand', 'temperature_2m']]\n",
    "\n",
    "            create_directories([os.path.dirname(self.config.label_encoder)])\n",
    "            save_bin(le, self.config.label_encoder)\n",
    "\n",
    "            logger.info(\"Basic preprocessing completed.\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def feature_engineering(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        try:\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce', utc=True)\n",
    "\n",
    "            df['hour'] = df['date'].dt.hour\n",
    "            df['day_of_week'] = df['date'].dt.dayofweek\n",
    "            df['month'] = df['date'].dt.month\n",
    "            df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "\n",
    "            holidays = calendar().holidays(start=df['date'].min(), end=df['date'].max())\n",
    "            df['is_holiday'] = df['date'].isin(holidays).astype(int)\n",
    "\n",
    "            logger.info(\"Feature engineering completed.\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    def train_test_splitting(self) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        try:\n",
    "            df = self.feature_engineering(self.basic_preprocessing())\n",
    "            df.sort_values(\"date\", inplace=True)\n",
    "\n",
    "            cutoff = pd.to_datetime(self.config.cutoff_date, utc=True)\n",
    "\n",
    "            train_df = df[df['date'] < cutoff].reset_index(drop=True)\n",
    "            test_df = df[df['date'] >= cutoff].reset_index(drop=True)\n",
    "\n",
    "            logger.info(f\"Train size: {train_df.shape}, Test size: {test_df.shape}\")\n",
    "            return train_df, test_df\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "        \n",
    "    def _get_cutoff_indices(self, df: pd.DataFrame, input_seq_len: int, step_size: int):\n",
    "        stop = len(df) - input_seq_len - 1\n",
    "        return [(i, i + input_seq_len, i + input_seq_len + 1) for i in range(0, stop, step_size)]\n",
    "        \n",
    "    def transform_ts_data_into_features_and_target(self, ts_data: pd.DataFrame) -> tuple[pd.DataFrame, pd.Series]:\n",
    "            \n",
    "            assert set(['date', 'demand', 'sub_region_code', 'temperature_2m']).issubset(ts_data.columns)\n",
    "\n",
    "            region_codes = ts_data['sub_region_code'].unique()\n",
    "            features = pd.DataFrame()\n",
    "            targets = pd.DataFrame()\n",
    "\n",
    "            input_seq_len = self.config.input_seq_len\n",
    "            step_size = self.config.step_size\n",
    "\n",
    "            for code in tqdm.tqdm(region_codes, desc=\"Transforming TS Data\"):\n",
    "                ts_one = ts_data[ts_data['sub_region_code'] == code].sort_values(by='date')\n",
    "                indices = self._get_cutoff_indices(ts_one, input_seq_len, step_size)\n",
    "\n",
    "                x = np.zeros((len(indices), input_seq_len), dtype=np.float64)\n",
    "                y = np.zeros((len(indices)), dtype=np.float64)\n",
    "                date_hours, temps = [], []\n",
    "\n",
    "                for i, (start, mid, end) in enumerate(indices):\n",
    "                    x[i, :] = ts_one.iloc[start:mid]['demand'].values\n",
    "                    y[i] = ts_one.iloc[mid]['demand']\n",
    "                    date_hours.append(ts_one.iloc[mid]['date'])\n",
    "                    temps.append(ts_one.iloc[mid]['temperature_2m'])\n",
    "\n",
    "                features_one = pd.DataFrame(\n",
    "                    x,\n",
    "                    columns=[f'demand_prev_{i+1}_hr' for i in reversed(range(input_seq_len))]\n",
    "                )\n",
    "                features_one['date'] = date_hours\n",
    "                features_one['sub_region_code'] = code\n",
    "                features_one['temperature_2m'] = temps\n",
    "\n",
    "                targets_one = pd.DataFrame(y, columns=['target_demand_next_hour'])\n",
    "\n",
    "                features = pd.concat([features, features_one], ignore_index=True)\n",
    "                targets = pd.concat([targets, targets_one], ignore_index=True)\n",
    "\n",
    "            return features, targets['target_demand_next_hour']\n",
    "\n",
    "    def preprocess_features(self, train_df: pd.DataFrame, test_df: pd.DataFrame):\n",
    "        try:\n",
    "            validation_status = self.check_status()\n",
    "\n",
    "            if not validation_status:\n",
    "                logger.error(\"Data validation failed. Aborting data transformation.\")\n",
    "                return None\n",
    "\n",
    "            logger.info(\"Validation PASSED. Proceeding with feature transformation...\")\n",
    "\n",
    "            # Transform to supervised format\n",
    "            train_x, train_y = self.transform_ts_data_into_features_and_target(train_df)\n",
    "            test_x, test_y = self.transform_ts_data_into_features_and_target(test_df)\n",
    "\n",
    "            # Drop 'date' before saving as .npy\n",
    "            train_x_npy = train_x.drop(columns=[\"date\"], errors=\"ignore\")\n",
    "            test_x_npy = test_x.drop(columns=[\"date\"], errors=\"ignore\")\n",
    "\n",
    "            # Save numpy arrays\n",
    "            np.save(self.config.x_transform, train_x_npy.values)\n",
    "            np.save(self.config.y_transform, train_y.values)\n",
    "\n",
    "            logger.info(f\"Shapes - Train X: {train_x_npy.shape}, Train Y: {train_y.shape}, test X: {test_x_npy.shape}, Test Y: {test_y.shape}\")\n",
    "\n",
    "            # Save CSVs for inspection\n",
    "            train_x.to_csv(self.config.train_features, index=False)\n",
    "            train_y.to_csv(self.config.train_target, index=False)\n",
    "            test_x.to_csv(self.config.test_features, index=False)\n",
    "            test_y.to_csv(self.config.test_target, index=False)\n",
    "\n",
    "            logger.info(\"Feature transformation and saving completed.\")\n",
    "            return (train_x, train_y), (test_x, test_y)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d60c4364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07 14:46:41,475: INFO: helpers: yaml file: config_file\\config.yaml loaded successfully]\n",
      "[2025-07-07 14:46:41,479: INFO: helpers: yaml file: config_file\\params.yaml loaded successfully]\n",
      "[2025-07-07 14:46:41,482: INFO: helpers: yaml file: config_file\\schema.yaml loaded successfully]\n",
      "[2025-07-07 14:46:41,484: INFO: helpers: created directory at: artifacts]\n",
      "[2025-07-07 14:46:41,487: INFO: helpers: created directory at: data_transformation]\n",
      "[2025-07-07 14:46:41,603: INFO: helpers: created directory at: artifacts\\data_transformation]\n",
      "[2025-07-07 14:46:41,603: INFO: helpers: binary file saved at: artifacts\\data_transformation\\label_encoder.pkl]\n",
      "[2025-07-07 14:46:41,603: INFO: 1862591776: Basic preprocessing completed.]\n",
      "[2025-07-07 14:46:41,635: INFO: 1862591776: Feature engineering completed.]\n",
      "[2025-07-07 14:46:41,650: INFO: 1862591776: Train size: (0, 9), Test size: (50809, 9)]\n",
      "[2025-07-07 14:46:41,650: INFO: 1862591776: Data validation status: True]\n",
      "[2025-07-07 14:46:41,650: INFO: 1862591776: Validation PASSED. Proceeding with feature transformation...]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming TS Data: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "CustomException",
     "evalue": "Exception in C:\\Users\\Javith Naseem\\AppData\\Local\\Temp\\ipykernel_9560\\2656274326.py, line 6: Exception in C:\\Users\\Javith Naseem\\AppData\\Local\\Temp\\ipykernel_9560\\1862591776.py, line 144: 'target_demand_next_hour'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 144\u001b[0m, in \u001b[0;36mDataTransformation.preprocess_features\u001b[1;34m(self, train_df, test_df)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Transform to supervised format\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m train_x, train_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_ts_data_into_features_and_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m test_x, test_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_ts_data_into_features_and_target(test_df)\n",
      "Cell \u001b[1;32mIn[7], line 131\u001b[0m, in \u001b[0;36mDataTransformation.transform_ts_data_into_features_and_target\u001b[1;34m(self, ts_data)\u001b[0m\n\u001b[0;32m    129\u001b[0m     targets \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([targets, targets_one], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features, \u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtarget_demand_next_hour\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mf:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n",
      "File \u001b[1;32mf:\\Program Files\\anaconda\\envs\\WattPredictor\\lib\\site-packages\\pandas\\core\\indexes\\range.py:418\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m--> 418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'target_demand_next_hour'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCustomException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      5\u001b[0m     train_df, test_df \u001b[38;5;241m=\u001b[39m data_transformation\u001b[38;5;241m.\u001b[39mtrain_test_splitting()\n\u001b[1;32m----> 6\u001b[0m     (train_x, train_y), (test_x, test_y) \u001b[38;5;241m=\u001b[39m \u001b[43mdata_transformation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[7], line 167\u001b[0m, in \u001b[0;36mDataTransformation.preprocess_features\u001b[1;34m(self, train_df, test_df)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CustomException(e, sys)\n",
      "\u001b[1;31mCustomException\u001b[0m: Exception in C:\\Users\\Javith Naseem\\AppData\\Local\\Temp\\ipykernel_9560\\1862591776.py, line 144: 'target_demand_next_hour'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCustomException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     (train_x, train_y), (test_x, test_y) \u001b[38;5;241m=\u001b[39m data_transformation\u001b[38;5;241m.\u001b[39mpreprocess_features(train_df, test_df)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CustomException(\u001b[38;5;28mstr\u001b[39m(e), sys)\n",
      "\u001b[1;31mCustomException\u001b[0m: Exception in C:\\Users\\Javith Naseem\\AppData\\Local\\Temp\\ipykernel_9560\\2656274326.py, line 6: Exception in C:\\Users\\Javith Naseem\\AppData\\Local\\Temp\\ipykernel_9560\\1862591776.py, line 144: 'target_demand_next_hour'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    train_df, test_df = data_transformation.train_test_splitting()\n",
    "    (train_x, train_y), (test_x, test_y) = data_transformation.preprocess_features(train_df, test_df)\n",
    "\n",
    "except Exception as e:\n",
    "    raise CustomException(str(e), sys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WattPredictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
